<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SQL快速参考]]></title>
    <url>%2F20180630%2Fquick_sql%2F</url>
    <content type="text"><![CDATA[Structured Query Language，结构化查询语言 基础12345678910111213SHOW DATABASES;USE database_name;SHOW COLUMNS FROM table_name; # 创建用户CREATE USER 'chen'@'%' IDENTIFIED [WITH mysql_native_password] BY 'chen123';# 检查用户SELECT user, host, plugin, authentication_string FROM mysql.user\G;# 授权GRANT ALL PRIVILEGES ON *.* TO 'chen'@'%';GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER ON *.* TO 'chen'@'%';# 查看权限SHOW GRANTS for 'chen'@'%'; SELECT1234567891011121314151617SELECT column_name(s)FROM table_name;SELECT [DISTINCT] column_name(s)FROM table_nameWHERE column_name operator value[AND|OR column_name operator value]ORDER BY column_name(s) ASC|DESC[LIMIT number];SELECT column_name(s)FROM table_nameWHERE column_name IN (value1,value2,...);SELECT column_name(s)FROM table_nameWHERE column_name BETWEEN 'A' AND 'H'; LIKE1234567891011121314SELECT column_name(s)FROM table_nameWHERE column_name LIKE pattern;# pattern'%a' # 以a结尾的数据'a%' # 以a开头的数据'%a%' # 含有a的数据'_a_' # 三位且中间字母是a的'_a' # 两位且结尾字母是a的'a_' # 两位且开头字母是a的# REGEXPSELECT * FROM WebsitesWHERE name REGEXP '^[A-H]'; INSERT123456789101112INSERT INTO table_nameVALUES (value1, value2, ...);INSERT INTO table_name (column_name1, column_name2, ...)VALUES(value1, value2, ...);INSERT INTO table2(column_name(s))SELECT column_name(s)FROM table1; UPDATE123UPDATE table_nameSET column1=value1, column2=value2, ...WHERE some_column=some_value; DELECT12345678DELECT FROM table_nameWHERE some_column=some_value;DELECT * FROM table_name;TRUNCATE table_name;DROP table_name; JOIN1234SELECT column_name(s)FROMtable1 INNER|LEFT|RIGHT|FULL JOIN table2ON table1.column_name=table2.column_name;\ UNION123SELECT column_name(s) FROM table1UNION [ALL]SELECT column)name(s) FROM table2; CREATE TABLE123456789101112CREATE TABLE persons( PersonID int NOT NULL AUTO_INCREMENT, StudentID int, LastName varchar(255), FirstName varchar(255), Address varchar(255) DEFAULT 'GuangZhou', CHECK (PersonID&gt;0), UNIQUE (StudentID), PRIMARY KEY (PersonID), FOREIGN KEY (StudentID) REFERENCES student(StudentID) ); ALTER TABLE1234567891011ALTER TABLE personsADD|DROP UNIQUE (PersonID);ALTER TABLE personsDROP PRIMARY KEY;ALTER TABLE personsADD CHECK (PersonID&gt;0);ALTER TABLE personsALTER Address SET DEFAULT 'GuangZhou'; VIEW1234CREATE VIEW view_name ASSELECT column_name(s)FROM table_nameWHERE condition; 函数12345678SELECT AVG(column_name) FROM table_name;SELECT COUNT(column_name) FROM table_name;SELECT column_name, aggregate_function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name; PyMySQL fetchone(): 该方法获取下一个查询结果集。结果集是一个对象 fetchall(): 接收全部的返回结果行. rowcount: 这是一个只读属性，并返回执行execute()方法后影响的行数。 重要函数 cursor(cursor = None): 创建一个游标 commit(): 事务提交，如果没有设为自动提交，则每次操作后必须提交事务，否则操作无效。 rollback(): 操作出错时，可以用这个函数回滚到执行事务之前 close(): 关闭连接 123456789101112# pymysql不支持caching_sha2_password加密，需指定mysql_native_password# create user 'wang'@'localhost' identified with mysql_native_password by 'wang123';# user需要权限# grant all privileges on *.* to 'wang'@'localhost';import pymysqldb = pymysql.connect(host='localhost', user='wang', password='wang123', db='world')cursor = db.cursor()cursor.execute('SELECT VERSION()')# db.commit() # 像数据库提交data = cursor.fetchone()print ('Database version : %s ' % data)db.close() 备忘1where add_time &gt;= UNIX_TIMESTAMP(to_date('20180101', 'yyyymmdd')) 更多函数 日期时间 to_date, 将字符类型按一定格式转化为日期类型, to_date(‘2004-11-27’, ‘yyyy-mm-dd’) to_char, 将日期类型转化为字符类型, to_char(sysdate, ‘yyyymmdd’) 12# 星期几to_char(to_date('2002-08-26','yyyy-mm-dd'),'day') date_format(date, format) from_unixtime(inux_timestamp, format) unix_timestamp(date) 数据类型转化，CAST (expression AS data_type)，SELECT CAST(‘12.5’ AS decimal(9,2)) # 精度9，小数点位数2 TRIM()，移除头尾空白 正则，select regexp_substr(a, ‘[0-9]+’)]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[江南烟雨——杭州]]></title>
    <url>%2F20180623%2Fhangzhou2018%2F</url>
    <content type="text"><![CDATA[杭州游玩 交通 &amp; 行程交通 机场大巴滨江线 沿途站点：海外创业人才基地——华美达酒店——地铁江陵路站——杭州萧山国际机场 机场发车时间：08：20、09：20、10：00、10：50、11：20、12：50、14：00、15：00、16：20、17：50、 18：50、20：00 滨江发车时间：06：00、07：30、08：30、09：20、10：00、11：20、12：30、13：40、14：50、 16：00、 17：20、18：40 景点 灵隐寺 开放时间：每天7:00-18:15 灵隐寺在飞来峰内，飞来峰门票：￥45，灵隐寺门票：￥30 断桥残雪- 西湖音乐喷泉 时间：每天晚上七点、八点 行程 Day1 断桥残血(10分钟)—&gt;白堤(20分钟)—&gt;孤山(1-2小时)—&gt;曲院风荷(1小时)—&gt;苏堤春晓(30分钟)—&gt;花港观鱼(1小时)—&gt;吴山广场(1小时)—&gt;西湖音乐喷泉(10分钟) Day2 飞来峰(1小时)—&gt;灵隐寺(1-2小时)—&gt;浴鹄湾(1小时)—&gt;六和听涛/六合塔(1小时)—&gt;河坊街(1-2小时)]]></content>
      <tags>
        <tag>走走看看</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224N笔记-Lecture1:Introduction]]></title>
    <url>%2F20180612%2Fcs224n-1%2F</url>
    <content type="text"><![CDATA[What is Natural Language Processing？ What’s special about human language? What’s Deep Learning? Why is NLP hard？ Deep NLP = Deep Learning + NLP What is Natural Language Processing？ 自然语言处理是计算机科学、人工智能以及语言学的交叉学科 自然语言处理的目标是让计算机理解人类语言，以完成有意义的任务，如机器翻译或QA等。 NLP Levels speech or text annlysis morphological analysis，形态分析 syntactic analysis，句法分析 semantic interpretation，语义分析 discourse processing，对话处理（理解上下文） What’s special about human language? 人类语言是一种离散/符号/类别信号系统 What’s Deep Learning? 深度学习是机器学习的一种 机器学习需要人工设计特征，然后把特征交给某个机器学习算法，机器为这些特征调整找到合适的权值，以生成最合适的模型；事实上，在这一过程中，是人类在学习，而机器只不过是解数值优化的题目而已。 Machine Learning in Practice = Describing your data with features a computer can understand (Domain specific, requires Ph.D. level talent) + Learning algorithm (Optimizing the weights on features) 表征学习（Representation Learning）是指通过对原始数据的学习，自动生成特征。 深度学习是表征学习的一部分，使用多层的表征学习（learned representations），故称为深度学习。 Reasons for Exploring Deep Learning 人工构建特征往往过去具体，而且需要大量时间去设计和验证 学习特征自适应能力强，而且学习速度快 能处理监督以及无监督问题 使人们兴奋的最大原因是，it work！效果优于传统机器学习 大量训练数据 CPU/GPU硬件资源的提升 Why is NLP hard？ 人类语言充满歧义 人类语言非常简练，省略了大量背景知识，not saying many thing The Pope’s baby steps on gays Deep NLP = Deep Learning + NLP近年来的研究进展 Levels：语音、词汇、语法、语义 Tools：词性标注、命名实体识别、句法/语义分析 Applications：机器翻译、情感分析、客服系统、问答系统 语义 Semantics 传统：Lambda calculus，手写大量规则 DL：每个短语、句子、逻辑表述都是向量 情感分析 传统：构建情感极性词典 QA 传统：大量逻辑规则 机器翻译 传统：在许多层级上进行尝试，试图找到一种世界通用的”国际语“作为翻译桥梁 DL：以Vector为翻译桥梁]]></content>
      <categories>
        <category>CS224N学习笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime使用指南]]></title>
    <url>%2F20180412%2Fsublime_using%2F</url>
    <content type="text"><![CDATA[个人Sublime使用记录 侧边显示文件结构 View-Side Bar-Show Side Bar File-Open Folder 导入了文件夹才会有文件夹的显示 安装Package Controlpython环境配置 安装SublimeRePL Preferences-Package Control 键入install packages 稍等片刻，键入SublimeRePL 等待安装即可 键位绑定 Preferences-Key Bindings123456789101112131415161718 &#123; "keys": ["f1"], "caption": "SublimeREPL: Python", "command": "run_existing_window_command", "args": &#123; "id": "repl_python", "file": "config/Python/Main.sublime-menu" &#125;&#125;,&#123; "keys": ["shift+enter"], "caption": "SublimeREPL: Python - RUN current file", "command": "run_existing_window_command", "args": &#123; "id": "repl_python_run", "file": "config/Python/Main.sublime-menu" &#125;&#125; markdown支持 安装SublimeRePL Preferences-Package Control 键入install packages 稍等片刻，键入Markdown Preview 等待安装即可 键位绑定 Preferences-Key Bindings12345678&#123; "keys": ["f2"], "command": "markdown_preview", "args": &#123; "target": "browser", "parser":"markdown" &#125;&#125; OneNote更友好的代码显示 方法很多，详见知乎 个人觉得最好的方法Sublime+SublimeHighlight 安装Sublime，好用的编辑器，不多说了 安装Package Control 安装SublimeHighlight Preferences-Package Control 键入Add Repository，用来添加插件安装源 地址栏输入https://github.com/n1k0/SublimeHighlight/tree/python3 Preferences-Package Control 键入install packages 稍等片刻，键入SublimeHighlight 等待安装即可 配置 Preference-Package Setting-Sublime Highlight-User Setting123456&#123; "theme": "monokai", "linenos": "inline", # 显示行号，不想显示设为false "noclasses": true, "fontface": "Menlo"&#125; 选中代码，右键Copy as HTML OneNote中新建1x1表格，复制进去即可 最后也可以修改表格底色 Python的Tab键换成四个空格 Preferences-setting 添加&quot;translate_tabs_to_spaces&quot;: true Python代码自动补全 参考：让你用sublime写出最完美的python代码–windows环境]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习知识要点记录]]></title>
    <url>%2F20180402%2Foffer%2F</url>
    <content type="text"><![CDATA[机器学习基本概念 LR SVM K-Means 决策树 随机森林 Ensemble Learning ARIMA 梯度下降 LSTM CNN NLP 机器学习基本概念 经验误差&泛化误差 学习器在训练集上的误差称为训练误差或经验误差，在新样本上的误差称为泛化误差 显然，我们希望得到泛化误差小的学习器 然而，我们事先并不知道新样本是什么样，实际能做的是努力使经验误差最小化，同时需要对过拟合进行一定处理 MSE、RMSE、MRE、MAE、MAPE MSE：均方误差 RMSE：均方根误差，针对异常值更加敏感；衡量预测值与真值之间的偏差，对误差去了平方，使得计算结果与较大值更相关，公式$RMSE=\sqrt{\frac{1}{n}\sum _{t=1}^n(|A_t-F_t|)^2}$ MRE：平均相对误差 MAE：平均绝对误差 MAPE：平均绝对百分比误差，不仅考虑了预测值与真实值的误差，还考虑了误差与真实值之间的比例，公式$MAPE=\frac{100}{n}\sum _{t=1}^n|\frac{A_t-F_t}{A_t}|$ other K-Means 什么是K-Means K-Means是一种聚类算法，基本思想是对于给定的类别数目k，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改变后的划分方案都比前一次好 K-Means具体做法 选择初始的k个类别中心 将每个样本标记为最近的类别 更新类别中心，将类别中心更新为该类别中所有样本的均值 重复前面两步，直到类别中心的变化小于某阈值或者迭代一定轮数 K-Means的优缺点 简单、快速 对高斯分布的数据集，效果较好 K需要预先定义 均值不可计算时无法使用 初值敏感 不适用于发现非凸或者大小差别很大的簇 对噪声和孤立点敏感 K值的确定 经验 又放回抽样，对不同的K值做两次聚类，计算相似性 计算平均轮廓系数 对不同的K值计算平方误差和（SSE），随着K的增大，SSE是肯定会降低的，找到那个开始降低不明显的点 初始化聚类中心 K-Means++ 先计算整体样本中心，然后从中心点出发，由近至远均匀采样 划分区域，区域中心未聚类中心 先用层次聚类初始聚类，再进行K-Means K-Means++算法 随机选取第一个聚类中心 计算每个点到最近聚类中心的距离D(x) 选择D(x)最大的点作为下一个聚类中心 重复上面步骤，直到得到k个聚类中心 决策树 什么是决策树 决策树是一种树形结构学习器 从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点 如此递归地对实例进行测试并分配，直到达到叶节点 由决策树的根节点到叶节点的每一条路径就构建除了一条规则 决策树学习本质上是从训练集中归纳出一组分类规则 决策树的三个步骤 特征选择 决策树的生成 决策树的修剪 决策树终止条件 节点样本集合为空 节点样本属于同一类别 节点属性集合为空 熵、条件熵、信息增益、基尼系数 基尼系数$Gini(p) = - \sum _{k=1}^Kp_k(1-p_k) = 1 - \sum ^Kp_k^2$ 熵：随机变量的不确定性的度量$H(X) = -\sum _{i=1}^np_ilogp_i$ 信息增益：g(D,A) = H(D) - H(D|A)，H(D)表示对数据集D进行分类的不确定性；H(D|A)表示特征A给定的条件下对数据集D分类的不确定性；信息增益则表示，由于特征A而使得数据集D的分类不确定性减少的程度 ID3、C4.5、CART ID3：基于信息增益的大小来逐层确定分类特征，偏向于选择取值多的特征 C4.5：先从候选特征中找出信息增益高于平均水平的特征，再从中选择信息增益比最高的 CART：分类树用基尼系数最小化，回归树用平方误差最小化 ID3的具体过程 从跟节点开始，对节点计算所有可能的特征信息增益 选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点 再对子节点递归地调用以上方法，构建决策树 直到所有特征的信息增益均很小或没有特征可以选择为止 决策树怎么剪枝 极小化决策树整体损失函数 决策树的优缺点 树形，非线性，简单，易于理解 能处理数据确实问题 能处理多分类问题 能同时处理连续型变量与离散型变量 缺点：容易过拟合，启发式 CART特点、剪枝、对缺失值的处理 每个节点采用二叉分裂，分裂准则采用基尼系数或者平方误差最小 剪枝：从生成算法产生的决策树底端开始不断剪枝，直到根节点，形成一个T0, T1, …, Tn的子序列 缺失值：训练的过程中保存替代特征，对于缺失数据，用替代特征进行划分 C4.5对缺失值的处理 划分点选择：未缺失样本的信息增益乘以为缺失样本的占比 怎么划分：让同一样本以不同的概率划入到不同的子节点中 分类与回归树的区别 分类树使用信息增益/信息增益比/基尼系数进行划分节点 回归树使用均方误差来划分节点 预剪枝与后剪枝 预剪枝：在决策树生成过程中，对每个节点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点 先从训练集生成一颗完整的决策树，然后从底向上地对非叶节点进行考察，若将该节点对应地子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点 随机森林 什么是随机森林 随机森林是以决策树为基学习器构建Bagging集成 进一步在决策树的训练过程中引入了随机特征组合 具体地说，就是在决策树每次进行最优划分特征选择时，不考虑所有的特征，而只考虑随机的特征子空间或者随机特征组合 随机森林的优点 高效并行训练 决策树的所有优点 降低了过拟合的可能 可用评估特征的重要性，做特征选择 怎么自我验证 随机森林采用了bagging的方式，每次训练大约有1/3的数据可以用于做验证 对于每个样本xn，都有部分子决策树没有使用该样本进行训练，我们将这些子树拿出来就组成了$G_n^− (x_n)$，再求一下误差值，最终将所有的$G_n^− (x_n)$综合起来 特征重要度衡量 将OOB的特征值进行重排，对模型效果影响是否显著 集成学习 什么是集成学习 对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好 什么是Boosting Boosting就是从弱学习算法出发，反复学习，得到一系列弱学习器，然后组合这些弱学习器得到一个强学习器 什么是AdaBoost 先从初始训练集训练出一个基学习器，在根据基学习器的表现对训练样本分布进行调整，使得之前错的训练样本在后续受到更多关注，也就是提高那些被前一轮分类器错误分类样本的权值，而降低那些被正确分类样本的权值 最终采用加权多数表决进行综合 什么是Gradient_Boosting 通过迭代拟合损失函数的负梯度值，让损失函数持续下降 每次迭代先求得关于累加函数F(x)的负梯度(- gradient)，然后通过弱学习器f(x)去拟合该负梯度，然后将弱学习器f累加到F得到新的F 当损失函数是平方损失时，该负梯度值也就是残差，模型退化为回归提升模型 什么是Bagging 基于bootstrap sampling：从m样本中，有放回地取出m个样本 构建多个学习器，并集成 什么是XGBoost 一个GBDT大规模并行开源框架 GBDT是一种Boosting Boosting是… GBDT与提升树的区别 提升树拟合的是残差，损失函数是均方误差；GBDT拟合的是负梯度值 GBDT中引入了学习率 GBDT如何转换为分类问题 二分类：比较最终的概率大小，区分属于正类还是负类 多分类：生成k棵回归数，每棵树的叶节点表示某一类样本的得分，样本最终对各类的概率由softmax对每个得分归一化处理得到 XGBoost与GBDT的区别 传统的GBDT以CART树为基学习器，XGBOOST还支持线性分类器，这时候的XGBOOST就相当于加了正则项的逻辑回归或者线性回归 XGBOOST在代价函数中加入了正则项，用于控制模型的复杂度；L1可以控制叶节点的个数，L2可以控制叶节点上的样本数目 传统GBDT在优化的时候只用到了一阶导数，XGBOOST则对代价函数进行二阶泰勒展开，同时利用一阶和二阶导信息计算 列抽样：XGBOOST借鉴了随机森林的做法，对特征抽样再拟合，不仅防止过拟合，还能减少计算 并行结算：XGBOOST在特征粒度上开多个线程同时计算它们各自的最佳信息增益 ARIMA 什么是ARIMA模型 稳定的标准 平稳的定义 平稳的序列通常具有短期相关性 如何判断是否平稳]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解stateful LSTM]]></title>
    <url>%2F20180116%2Funderstanding-Stateful-LSTM%2F</url>
    <content type="text"><![CDATA[原文Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras 任务描述：学习字母表 【1】Naive LSTM for Learning One-Char to One-Char Mapping 使用t时刻序列值，预测t+1时刻序列值 输入维度1，输入用one hot编码 效果不好，原因： LSTM没有可以考虑的上下文 每批训练，Keras默认重置网络状态 本质上是将LSTM单元用作多层感知机，是对LSTM的误用 【2】Naive LSTM for a Three-Char Feature Window to One-Char Mapping 使用t-n, t-n+1, …, t时刻序列值为特征，预测t+1时刻序列值 性能小幅提升，但同样不好 本质上仍是将LSTM单元用作多层感知机，只不过通过window method来提供上下文，是对LSTM的误用 事实上，序列特征是具有time steps的一个特征，而不是one time step的多个特征 【3】Naive LSTM for a Three-Char Time Step Window to One-Char Mapping 使用t-n, t-n+1, …, t时刻序列值为time steps特征，预测t+1时刻序列值 正确率100%，可以很好的学习到字母表 但只能通过前n个序列值预测第n+1个序列值，而不是完整的学习了整张字母表 事实上，通过一个足够大的多层感知机也似乎能完成 【4】LSTM State Within A Batch 使用整个序列训练模型，输入t时刻序列值，预测t+1时刻序列值 LSTM是有状态的，但在每批训练之后，Keras会默认重置。这意味着，如果我们使用一个足够大的batch，一次训练所有数据，则LSTM会很好的考虑上下文信息 正确率100%，可以输入任意字母预测下一个字母 问题在于，每批训练都要给网络喂全量数据 【5】Stateful LSTM for a One-Char to One-Char Mapping 使用整个序列训练模型，输入t时刻序列值，预测t+1时刻序列值 Ideally, we want to expose the network to the entire sequence and let it learn the inter-dependencies, rather than us define those dependencies explicitly in the framing of the problem. 上述通过stateful LSTM来实现，这才是LSTM的正确用法 手动训练每一期（epoch），每期分为多批，每批训练状态会被记录并传递到下一批，直到本期训练完成后重置状态 正确率理论100%（增加epoch可提高正确率） 冷启动问题，网络的第一个输入不能被正确预测 类似于【3】，但不是人为确定time step，而是由网络自主学习 【6】LSTM with Variable-Length Input to One-Char Output 使用t时刻前可变序列值，预测t+1时刻序列值 定义输入最大time steps，不够最大长度的在前面补0 效果一般，但已经能通过任意长度的序列预测下一刻序列值 本质上就是【3】，输入改为可变长度]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】不同的截断反向传播]]></title>
    <url>%2F20171221%2FStyles-of-Truncated-Backpropagation%2F</url>
    <content type="text"><![CDATA[本文GitHub地址博客地址原文Styles of Truncated Backpropagation翻译带个人理解，建议啃原文 笔者在之前的博文Recurrent Neural Networks in Tensorflow中观察到，TensorFlow的截断反向传播方式与“误差最多反向传播n steps”不同。笔者将在这篇博文中基于TensorFlow实现不同的截断反向传播算法，并研究哪种方法更好。 结论是： 一个实现得很好、均匀分布的截断反向传播算法与全反向传播（full backpropagation）算法在运行速度上相近，而全反向传播算法在性能上稍微好一点。 初步实验表明，n-step Tensorflow-style 反向传播（with num_step=n）并不能有效地反向传播整个n steps误差。因此，如果使用Tensorflow-style 截断反向传播并且需要捕捉n-step依赖关系，则可以使用明显高于n的num_step以便于有效地将误差反向传播所需的n steps。 Differences in styles of truncated backpropagation假设要训练一个RNN，训练集为长度10,000的序列。如果我们使用非截断反向传播，那么整个序列会一次性地喂给网络，在时刻10,000的误差会一直传递到时刻1。这会导致两个问题： 误差反向传播这么多步的计算开销大 由于梯度消失，反向传播误差逐层变小，使得进一步的反向传播不再重要 可以使用“截断”反向传播解决这个问题，Ilya Sutskever博士这篇文章的2.8.6节有一个对截断反向传播很好的描述： “[Truncated backpropagation] processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps…” Tensorflow-style反向传播使用k1=k2(=num_steps)（详情请参阅Tensorflow Api）。本文提出的问题是：k1=1能否得到一个更好的结果。本文认为，“真”截断反向传播是：每次反向传播k2 steps都是传播了整个k2 steps。 举个例子解释这两种方法的不同。序列长度为49，反向传播截断为7 steps。相同的是，每个误差都会反向传播7 steps。但是，对于Tensorflow-style截断反向传播，序列会被切分成长度为7的7个子序列，并且只有7个误差被反向传播了7 steps。而对于“真”截断反向传播而言，42个误差能被反向传播7 steps就应该42个都反向传播7 steps。这就产生了不同，因为使用7-steps与1-steps更新权重明显不同。 为了可视化差异，下图表示的是序列长度为6，误差反向传播3 steps： 下图是Tensorflow-style截断反向传播在同一序列上的情况： 实验设计为了比较两种算法的性能，笔者实现了一个“真”截断反向传播算法，并与vanilla-RNN比较结果。vanilla-RNN是笔者在先前的博文Recurrent Neural Networks in Tensorflow I中使用的模型。不同的是，先前的网络在toy数据集上学习简单模式非常快，而本次提升了任务和模型的复杂性。这次任务是对ptb数据集进行语言建模，在基本的RNN模型中添加了一个embedding层和dropout层用以匹配任务的复杂性。 实验比较了每个算法20 epochs训练后验证集上的最佳性能，使用的是Adam Optimizer（初步实验它比其他Optimizer好），学习率设置为0.003，0.001和0.0003。 5-step truncated backpropagation True, sequences of length 20 TF-style 10-step truncated backpropagation True, sequences of length 30 TF-style 20-step truncated backpropagation True, sequences of length 40 TF-style 40-step truncated backpropagation TF-style 代码Imports and data generators12345678910111213import numpy as npimport tensorflow as tf%matplotlib inlineimport matplotlib.pyplot as pltfrom tensorflow.models.rnn.ptb import reader#data from http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgzraw_data = reader.ptb_raw_data('ptb_data')train_data, val_data, test_data, num_classes = raw_datadef gen_epochs(n, num_steps, batch_size): for i in range(n): yield reader.ptb_iterator(train_data, batch_size, num_steps) Model123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152def build_graph(num_steps, bptt_steps = 4, batch_size = 200, num_classes = num_classes, state_size = 4, embed_size = 50, learning_rate = 0.01): """ Builds graph for a simple RNN Notable parameters: num_steps: sequence length / steps for TF-style truncated backprop bptt_steps: number of steps for true truncated backprop """ g = tf.get_default_graph() # placeholders x = tf.placeholder(tf.int32, [batch_size, None], name='input_placeholder') y = tf.placeholder(tf.int32, [batch_size, None], name='labels_placeholder') default_init_state = tf.zeros([batch_size, state_size]) init_state = tf.placeholder_with_default(default_init_state, [batch_size, state_size], name='state_placeholder') dropout = tf.placeholder(tf.float32, [], name='dropout_placeholder') x_one_hot = tf.one_hot(x, num_classes) x_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, x_one_hot)] with tf.variable_scope('embeddings'): embeddings = tf.get_variable('embedding_matrix', [num_classes, embed_size]) def embedding_lookup(one_hot_input): with tf.variable_scope('embeddings', reuse=True): embeddings = tf.get_variable('embedding_matrix', [num_classes, embed_size]) embeddings = tf.identity(embeddings) g.add_to_collection('embeddings', embeddings) return tf.matmul(one_hot_input, embeddings) rnn_inputs = [embedding_lookup(i) for i in x_as_list] #apply dropout to inputs rnn_inputs = [tf.nn.dropout(x, dropout) for x in rnn_inputs] # rnn_cells with tf.variable_scope('rnn_cell'): W = tf.get_variable('W', [embed_size + state_size, state_size]) b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0)) def rnn_cell(rnn_input, state): with tf.variable_scope('rnn_cell', reuse=True): W = tf.get_variable('W', [embed_size + state_size, state_size]) W = tf.identity(W) g.add_to_collection('Ws', W) b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0)) b = tf.identity(b) g.add_to_collection('bs', b) return tf.tanh(tf.matmul(tf.concat(1, [rnn_input, state]), W) + b) state = init_state rnn_outputs = [] for rnn_input in rnn_inputs: state = rnn_cell(rnn_input, state) rnn_outputs.append(state) #apply dropout to outputs rnn_outputs = [tf.nn.dropout(x, dropout) for x in rnn_outputs] final_state = rnn_outputs[-1] #logits and predictions with tf.variable_scope('softmax'): W = tf.get_variable('W_softmax', [state_size, num_classes]) b = tf.get_variable('b_softmax', [num_classes], initializer=tf.constant_initializer(0.0)) logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs] predictions = [tf.nn.softmax(logit) for logit in logits] #losses y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)] losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logit,label) \ for logit, label in zip(logits, y_as_list)] total_loss = tf.reduce_mean(losses) """ Implementation of true truncated backprop using TF's high-level gradients function. Because I add gradient-ops for each error, this are a number of duplicate operations, making this a slow implementation. It would be considerably more effort to write an efficient implementation, however, so for testing purposes, it's OK that this goes slow. An efficient implementation would still require all of the same operations as the full backpropagation through time of errors in a sequence, and so any advantage would not come from speed, but from having a better distribution of backpropagated errors. """ embed_by_step = g.get_collection('embeddings') Ws_by_step = g.get_collection('Ws') bs_by_step = g.get_collection('bs') # Collect gradients for each step in a list embed_grads = [] W_grads = [] b_grads = [] # Keeping track of vanishing gradients for my own curiousity vanishing_grad_list = [] # Loop through the errors, and backpropagate them to the relevant nodes for i in range(num_steps): start = max(0,i+1-bptt_steps) stop = i+1 grad_list = tf.gradients(losses[i], embed_by_step[start:stop] +\ Ws_by_step[start:stop] +\ bs_by_step[start:stop]) embed_grads += grad_list[0 : stop - start] W_grads += grad_list[stop - start : 2 * (stop - start)] b_grads += grad_list[2 * (stop - start) : ] if i &gt;= bptt_steps: vanishing_grad_list.append(grad_list[stop - start : 2 * (stop - start)]) grad_embed = tf.add_n(embed_grads) / (batch_size * bptt_steps) grad_W = tf.add_n(W_grads) / (batch_size * bptt_steps) grad_b = tf.add_n(b_grads) / (batch_size * bptt_steps) """ Training steps """ opt = tf.train.AdamOptimizer(learning_rate) grads_and_vars_tf_style = opt.compute_gradients(total_loss, tf.trainable_variables()) grads_and_vars_true_bptt = \ [(grad_embed, tf.trainable_variables()[0]), (grad_W, tf.trainable_variables()[1]), (grad_b, tf.trainable_variables()[2])] + \ opt.compute_gradients(total_loss, tf.trainable_variables()[3:]) train_tf_style = opt.apply_gradients(grads_and_vars_tf_style) train_true_bptt = opt.apply_gradients(grads_and_vars_true_bptt) return dict( train_tf_style = train_tf_style, train_true_bptt = train_true_bptt, gvs_tf_style = grads_and_vars_tf_style, gvs_true_bptt = grads_and_vars_true_bptt, gvs_gradient_check = opt.compute_gradients(losses[-1], tf.trainable_variables()), loss = total_loss, final_state = final_state, x=x, y=y, init_state=init_state, dropout=dropout, vanishing_grads=vanishing_grad_list ) Some quick tests速度测试不出所料，由于执行了重复操作，实现的真BPTT速度很慢。一个有效的实现运行速度将与全反向传播大致相同。 123456reset_graph()g = build_graph(num_steps = 40, bptt_steps = 20)sess = tf.InteractiveSession()sess.run(tf.initialize_all_variables())X, Y = next(reader.ptb_iterator(train_data, batch_size=200, num_steps=40)) 12%%timeitgvs_bptt = sess.run(g['gvs_true_bptt'], feed_dict=&#123;g['x']:X, g['y']:Y, g['dropout']: 1&#125;) 10 loops, best of 3: 173 ms per loop 12%%timeitgvs_tf = sess.run(g['gvs_tf_style'], feed_dict=&#123;g['x']:X, g['y']:Y, g['dropout']: 1&#125;) 10 loops, best of 3: 80.2 ms per loop 梯度消失演示为了演示梯度消失问题，收集以下信息。如你所见，梯度很快就会消失，每一步都会减少3-4倍。 123456789101112vanishing_grads, gvs = sess.run([g['vanishing_grads'], g['gvs_true_bptt']], feed_dict=&#123;g['x']:X, g['y']:Y, g['dropout']: 1&#125;)vanishing_grads = np.array(vanishing_grads)weights = gvs[1][1]# sum all the grads from each loss nodevanishing_grads = np.sum(vanishing_grads, axis=0)# now calculate the l1 norm at each bptt stepvanishing_grads = np.sum(np.sum(np.abs(vanishing_grads),axis=1),axis=1)vanishing_grads array([ 5.28676978e-08, 1.51207473e-07, 4.04591049e-07, 1.55859300e-06, 5.00411124e-06, 1.32292716e-05, 3.94736344e-05, 1.17605050e-04, 3.37805774e-04, 1.01710076e-03, 2.74375151e-03, 8.92040879e-03, 2.23708227e-02, 7.23497868e-02, 2.45202959e-01, 7.39126682e-01, 2.19093657e+00, 6.16793633e+00, 2.27248211e+01, 9.78200531e+01], dtype=float32) 12for i in range(len(vanishing_grads) - 1): print(vanishing_grads[i+1] / vanishing_grads[i]) 2.86011 2.67573 3.85227 3.21066 2.64368 2.98381 2.97933 2.87237 3.0109 2.69762 3.25117 2.50782 3.23411 3.38913 3.01435 2.96422 2.81521 3.68435 4.30455 1plt.plot(vanishing_grads) Quick accuracy test一个完整检查，以确保真截断反向传播算法运行正确。 123456789101112131415161718# first test using bptt_steps &gt;= num_stepsreset_graph()g = build_graph(num_steps = 7, bptt_steps = 7)X, Y = next(reader.ptb_iterator(train_data, batch_size=200, num_steps=7))with tf.Session() as sess: sess.run(tf.initialize_all_variables()) gvs_bptt, gvs_tf =\ sess.run([g['gvs_true_bptt'],g['gvs_tf_style']], feed_dict=&#123;g['x']:X, g['y']:Y, g['dropout']: 0.8&#125;)# assert embedding gradients are the sameassert(np.max(gvs_bptt[0][0] - gvs_tf[0][0]) &lt; 1e-4)# assert weight gradients are the sameassert(np.max(gvs_bptt[1][0] - gvs_tf[1][0]) &lt; 1e-4)# assert bias gradients are the sameassert(np.max(gvs_bptt[2][0] - gvs_tf[2][0]) &lt; 1e-4) 实验123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566"""Train the network"""def train_network(num_epochs, num_steps, use_true_bptt, batch_size = 200, bptt_steps = 7, state_size = 4, learning_rate = 0.01, dropout = 0.8, verbose = True): reset_graph() tf.set_random_seed(1234) g = build_graph(num_steps = num_steps, bptt_steps = bptt_steps, state_size = state_size, batch_size = batch_size, learning_rate = learning_rate) if use_true_bptt: train_step = g['train_true_bptt'] else: train_step = g['train_tf_style'] with tf.Session() as sess: sess.run(tf.initialize_all_variables()) training_losses = [] val_losses = [] for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)): training_loss = 0 steps = 0 training_state = np.zeros((batch_size, state_size)) for X, Y in epoch: steps += 1 training_loss_, training_state, _ = sess.run([g['loss'], g['final_state'], train_step], feed_dict=&#123;g['x']: X, g['y']: Y, g['dropout']: dropout, g['init_state']: training_state&#125;) training_loss += training_loss_ if verbose: print("Average training loss for Epoch", idx, ":", training_loss/steps) training_losses.append(training_loss/steps) val_loss = 0 steps = 0 training_state = np.zeros((batch_size, state_size)) for X,Y in reader.ptb_iterator(val_data, batch_size, num_steps): steps += 1 val_loss_, training_state = sess.run([g['loss'], g['final_state']], feed_dict=&#123;g['x']: X, g['y']: Y, g['dropout']: 1, g['init_state']: training_state&#125;) val_loss += val_loss_ if verbose: print("Average validation loss for Epoch", idx, ":", val_loss/steps) print("***") val_losses.append(val_loss/steps) return training_losses, val_losses 结果1234567891011121314151617# Procedure to collect results# Note: this takes a few hours to runbptt_steps = [(5,20), (10,30), (20,40), (40,40)]lrs = [0.003, 0.001, 0.0003]for bptt_step, lr in ((x, y) for x in bptt_steps for y in lrs): _, val_losses = \ train_network(20, bptt_step[0], use_true_bptt=False, state_size=100, batch_size=32, learning_rate=lr, verbose=False) print("** TF STYLE **", bptt_step, lr) print(np.min(val_losses)) if bptt_step[0] != 0: _, val_losses = \ train_network(20, bptt_step[1], use_true_bptt=True, bptt_steps= bptt_step[0], state_size=100, batch_size=32, learning_rate=lr, verbose=False) print("** TRUE STYLE **", bptt_step, lr) print(np.min(val_losses)) Minimum validation loss achieved in 20 epochs: BPTT Steps 5 Learning Rate 0.003 0.001 0.0003 True (20-seq) 5.12 5.01 5.09 TF Style 5.21 5.04 5.04 BPTT Steps 10 Learning Rate 0.003 0.001 0.0003 True (30-seq) 5.07 5.00 5.12 TF Style 5.15 5.03 5.05 BPTT Steps 20 Learning Rate 0.003 0.001 0.0003 True (40-seq) 5.05 5.00 5.15 TF Style 5.11 4.99 5.08 BPTT Steps 40 Learning Rate 0.003 0.001 0.0003 TF Style 5.05 4.99 5.15 Discussion如你所见，当以相同的steps截断误差时，“真”截断反向传播算法似乎比Tensorflow-style好。但是，随着BPTT steps逐渐增加，这种优势逐渐减弱，当Tensorflow-style截断反向传播steps为序列长度时，这种优势便消失了，甚至反过来。 所以结论是： 一个实现得很好、均匀分布的截断反向传播算法与全反向传播（full backpropagation）算法在运行速度上相近，而全反向传播算法在性能上稍微好一点。 初步实验表明，n-step Tensorflow-style 反向传播（with num_step=n）并不能有效地反向传播整个n steps误差。因此，如果使用Tensorflow-style 截断反向传播并且需要捕捉n-step依赖关系，则可以使用明显高于n的num_step以便于有效地将误差反向传播所需地n steps。 Edit: 写完这篇博文之后，才发现关于不同的截断反向传播已经在 Williams and Zipser (1992), Gradient-Based Learning Algorithms for Recurrent Networks and Their Computation Complexity这篇论文中讨论。作者将本文中提到的“真”反向传播定义为“截断反向传播”，表示为：BPTT(n)/BPTT(n, 1)；而Tensorflow-style截断反向传播定义为“epochwise 截断反向传播”，表示为：BPTT(n, n)。同时允许“semi-epochwise”截断BPTT，which would do a backward pass more often than once per sequence, but less often than all possible times (i.e., in Ilya Sutskever’s language used above, this would be BPTT(k2, k1), where 1 &lt; k1 &lt; k2).【译者注：本人才疏学浅未理解】 在 Williams and Peng (1990), An Efficien Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories中，作者进行了类似本文的实验，并得出与本文类似的结论。Williams Peng写到“The results of these experiments have been that the success rate of BPTT(2h; h) is essentially identical to that of BPTT(h)”，也就是说，他比较了“真”截断h-steps反向传播与BPTT(2n, n)(这与截断2n-steps Tensorflow-style反向传播相似)，最终发现它们表现相近。]]></content>
      <categories>
        <category>翻译</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debug Myself]]></title>
    <url>%2F20171023%2Fdebug%2F</url>
    <content type="text"><![CDATA[记录个人编程之路上遇到的问题以及解决方案 Keras 【2017.10.26】ImportError: Failed to import pydot. You must install pydot and graphviz for pydotprint to work.Keras的Model visualization可以很方便可视化网络 123# Keras-2.0.8from keras.utils import plot_modelplot_model(model, to_file='model.png') 这个报错十分具有误导性…即使执行了pip install pydot；pip install pydot-ng；pip install graphviz还是有这个报错。报错的原因其实不在于pydot，跟python包没有关系，而是因为graphviz需要安装二进制执行文件（跟imagick类似），所以还需要去官网下一个graphviz安装包安装。双击运行之后一路next，最后将C:\Program Files (x86)\Graphviz2.38\bin加入系统环境变量。 12pip install pydotpip install pydot-ng # Keras 2.0.6之后只需安装pydot-ng 重启python即可 参考1：keras可视化遇到pydot&amp;graphviz无法导入问题 参考2：stackoverflow TensorFlow 【2017.10.27】Windows安装tensorflow-gpu 先安装cuda_8.0.44_win10-exe 解压cudnn-8.0-windows10-x64-v6.0.zip，并将三个文件复制到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0中的相应位置 最后pip install --ignore-installed --upgrade tensorflow-gpu安装TensorFlow 直接在官网下的是cuda v9.0，目前tensorflow暂不支持！ 参考：TensorFlow安装方法二【GPU环境配置部分】（Windows10 64位 cpu and gpu） 【2017.12.13】ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory 问题出在不能正确找到CUDA 在CUDA已经安装的前提下，在.bashrc文件中增加 1export LD_LIBRARY_PATH=/usr/local/cuda/lib64/ 如果需要在Pycharm中进行远程，则在run configuration中增加cuda lib的环境变量 【2018.05.11】升级Tensorflow tensorflow升级到1.8之后，cuda需要升级到9.0 下载cuda_9.0.176_win10.exe以及cudnn-9.0-windows10-x64-v7.1.zip 自定义安装cuda_9.0.176_win10.exe，只安装CUDA 解压cudnn-9.0-windows10-x64-v7.1.zip，并将三个文件复制到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0中的相应位置 在系统环境变量中删除之前版本的CUDA nvcc -V验证版本 目前tensorflow最新版本1.8.0只支持到CUDA9.0，如果想使用CUDA9.1，可以使用大神预编译好的版本，GitHUB地址。 Git 【2017.11.22】.gitignore文件 data: 匹配根目录以及所有子目录中名字为“data”的文件或者文件夹 data/: 匹配根目录以及所有子目录中名字为“data”的文件夹 data/*: 匹配根目录中名字为“data”的文件夹 */data/*: 匹配所有子目录中名字为“data”的文件夹 Pycharm 【2017.12.13】matplotlib显示远程服务器上图片从零开始：使用PyCharm和SSH搭建远程TensorFlow开发环境Python plotting on remote server using PyCharmPycharm远程调用Centos GUI程序,显示在windows上matplotlib绘图错误：TclError: no display name and no $DISPLA emmmm，进入正题，我误打误撞的解决方案。MobaXterm + pycharm 使用MobaXterm登陆服务器 echo $DISPLAY查看MobaXterm转发显示的number，如localhost:10.0 在pycharm的run configuration中增加DISPLAY的环境变量 XGBOOST​ 【2018.05.15】Ubuntupip install xgboost（有毒）]]></content>
      <categories>
        <category>程序员</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简单的信息统计页面Demo]]></title>
    <url>%2F20171008%2Fdjango-demo%2F</url>
    <content type="text"><![CDATA[Python + Django搭建简单的信息统计页面 GitHub占个坑先]]></content>
      <categories>
        <category>不务正业</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现自动挂机脚本]]></title>
    <url>%2F20170825%2Fauto-yys%2F</url>
    <content type="text"><![CDATA[使用Python + win32api实现简单自动鼠标点击 使用tkinter设计GUI界面并用pyinstaller打包 不知不觉肝阴阳师也快一年了，对这游戏真是又爱又恨，最近刚刚发布了PC版，突然很想尝试着写个脚本挂机，话不多说进入正题。 基础模拟点击简单的鼠标操作游戏挂机脚本，无非就是自动移动鼠标，自动点击，进行重复操作，所以，第一步就是如何控制鼠标 1234567891011121314import win32apiimport timedef move_click(x, y, t=0): # 移动鼠标并点击左键 win32api.SetCursorPos((x, y)) # 设置鼠标位置(x, y) win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN | win32con.MOUSEEVENTF_LEFTUP, x, y, 0, 0) # 点击鼠标左键 if t == 0: time.sleep(random.random()*2+1) # sleep一下 else: time.sleep(t) return 0# 测试move_click(30, 30) 当然，再后续操作中你可能需要获取屏幕分辨率，我只打算让脚本能在自己电脑上跑就满足了，所以没有实现适配不同分辨率 12def resolution(): # 获取屏幕分辨率 return win32api.GetSystemMetrics(0), win32api.GetSystemMetrics(1) 值得注意的是，一定要在管理员权限下的cmd中运行，否则点击无效 这个时候，你已经可以写个while循环，不停地点击屏幕上不同的几个点了，最基础的挂机脚本就实现了 使用PIL识别图像我们肯定不满足于机械式的连续点击，万一被封号呢…所以需要识别图像，再进行点击首先，就需要定位到阴阳师的窗口 12345678910import win32guidef get_window_info(): # 获取阴阳师窗口信息 wdname = u'阴阳师-网易游戏' handle = win32gui.FindWindow(0, wdname) # 获取窗口句柄 if handle == 0: # text.insert('end', '小轩提示：请打开PC端阴阳师\n') # text.see('end') # 自动显示底部 return None else: return win32gui.GetWindowRect(handle) get_window_info()函数返回阴阳师窗口信息(x1, y1, x2, y2)，(x1, y1)是窗口左上角的坐标，(x2, y2)是窗口右下角的坐标，代码中的text可以暂时忽略，这在后续GUI界面中用于输出提示信息。下面使用PIL获取游戏截图 123456789101112def get_posx(x, window_size): # 返回x相对坐标 return (window_size[2] - window_size[0]) * x / 870def get_posy(y, window_size): # 返回y相对坐标 return (window_size[3] - window_size[1]) * y / 520topx, topy = window_size[0], window_size[1]img_ready = ImageGrab.grab((topx + get_posx(500, window_size), topy + get_posy(480, window_size), topx + get_posx(540, window_size), topy + get_posy(500, window_size)))# 查看图片im_ready.show() 考虑到窗口大小不同，位置会有所偏移，这里使用屏幕上点的相对位置获取到关键位置的截图之后，计算图片的hash值 12345def get_hash(img): img = img.resize((16, 16), Image.ANTIALIAS).convert('L') # 抗锯齿 灰度 avg = sum(list(img.getdata())) / 256 # 计算像素平均值 s = ''.join(map(lambda i: '0' if i &lt; avg else '1', img.getdata())) # 每个像素进行比对,大于avg为1,反之为0 return ''.join(map(lambda j: '%x' % int(s[j:j+4], 2), range(0, 256, 4))) 将关键位置截图的hash值保存下来，下次脚本运行时，将截图hash值与原始hash值进行比对，判断是否相似。这里使用汉明距离进行计算，比较hash值中相同位置上不同元素的个数 123456def hamming(hash1, hash2, n=20): b = False assert len(hash1) == len(hash2) if sum(ch1 != ch2 for ch1, ch2 in zip(hash1, hash2)) &lt; n: b = True return b 准备工作做完了，下面就可以开心刷御灵了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def yu_ling(window_size): global is_start topx, topy = window_size[0], window_size[1] state = [] while is_start: # print 'start' # text.insert('end', 'start') time.sleep(0.5) img_ready = ImageGrab.grab((topx + get_posx(750, window_size), topy + get_posy(465, window_size), topx + get_posx(840, window_size), topy + get_posy(500, window_size))) if hamming(get_hash(img_ready), ready_hash, 10): state.append(0) move_click(topx + get_posx(740, window_size), topy + get_posy(380, window_size)) text.insert('end', strftime('%H:%M:%S', localtime()) + ' 点击准备\n') text.see('end') # 自动显示底部 time.sleep(15) continue img_success = ImageGrab.grab((topx + get_posx(400, window_size), topy + get_posy(320, window_size), topx + get_posx(470, window_size), topy + get_posy(400, window_size))) if hamming(get_hash(img_success), success_hash): time.sleep(2) state.append(1) text.insert('end', strftime('%H:%M:%S', localtime()) + ' 成功%d次\n' % state.count(1)) text.see('end') # 自动显示底部 move_click(topx + get_posx(730, window_size), topy + get_posy(380, window_size)) continue img_fail = ImageGrab.grab((topx + get_posx(560, window_size), topy + get_posy(340, window_size), topx + get_posx(610, window_size), topy + get_posy(390, window_size))) if hamming(get_hash(img_fail), fail_hash): time.sleep(2) state.append(2) text.insert('end', strftime('%H:%M:%S', localtime()) + ' 失败%d次\n' % state.count(2)) text.see('end') # 自动显示底部 move_click(topx + get_posx(720, window_size), topy + get_posy(380, window_size)) continue img_attack = ImageGrab.grab((topx + get_posx(615, window_size), topy + get_posy(350, window_size), topx + get_posx(675, window_size), topy + get_posy(375, window_size))) if hamming(get_hash(img_attack), yu_attack_hash): move_click(topx + get_posx(670, window_size), topy + get_posy(365, window_size)) text.insert('end', strftime('%H:%M:%S', localtime()) + ' 点击进攻\n') text.see('end') # 自动显示底部 state.append(3) if state[-6:] == [3]*6: text.insert('end', strftime('%H:%M:%S', localtime()) + ' 痴汉券可能不够了\n') text.see('end') # 自动显示底部 click() break continue 至此，我们已经可以通过管理员cmd运行脚本了。但这样的脚本运行起来比较麻烦，也没有好看的界面。接下来，我们将使用tkinter设计GUI界面，并用pyinstaller打包成.exe文件 GUItkintertkinter是Python内置的GUI设计界面，对小白来说容易上手，你也可以尝试用pyqt或者wx关于tkinter可以看一下莫烦教程 首先创建一个窗口，并设置必要信息12345678910111213141516import tkinter as tkfrom icon import imgwindow = tk.Tk() # 创建一个窗口window.title('奴良小轩v0.1')window.geometry('240x480+120+30') # 窗口的位置以及大小# 设置图标with open('tmp.ico', 'wb+') as fp: fp.write(base64.b64decode(img))window.iconbitmap('tmp.ico')os.remove('tmp.ico')# 设置图标label = tk.Label(window, font=('微软雅黑', 12), text='请将PC端阴阳师调节与小宝等高') # 显示一段文本label.pack() 设置图标默认情况下，窗口图标是红色的TK，想修改则使用.iconbitmap(path)方法，但是，在实际使用踩坑了。因为后面我会使用pyinstaller打包，因为找不到path路径运行程序会报错，找了好久才找到这个错误。解决方案是先将图标读取并写入ico.py文件，调用.iconbitmap(path)时读取ico.py，代码如下： 12345678import base64open_icon = open('yaodao.ico', 'rb')b64str = base64.b64encode(open_icon.read())open_icon.close()write_data = "img = '%s'" % b64strf = open('icon.py', 'w+')f.write(write_data)f.close() 功能选择1234567891011121314151617181920212223242526272829303132333435# Radiobutton #fun_var = tk.IntVar()fun_text = ''def print_selection(): global fun_text if fun_var.get() == 1: fun_text = '寮突破' elif fun_var.get() == 2: fun_text = '御灵、业原火' elif fun_var.get() == 3: fun_text = '魂十队员(未完成)' elif fun_var.get() == 4: fun_text = '魂十队长(未完成)' elif fun_var.get() == 5: fun_text = '狗粮队员(未完成)' label.config(text='功能选择： ' + fun_text)rb1 = tk.Radiobutton(window, text='寮突破', font=('微软雅黑', 10), variable=fun_var, value=1, command=print_selection)rb1.place(x=15, y=30)rb2 = tk.Radiobutton(window, text='御灵、业原火', font=('微软雅黑', 10), variable=fun_var, value=2, command=print_selection)rb2.place(x=15, y=60)rb3 = tk.Radiobutton(window, text='魂十队员', font=('微软雅黑', 10), variable=fun_var, value=3, command=print_selection)rb3.place(x=15, y=90)rb4 = tk.Radiobutton(window, text='魂十队长', font=('微软雅黑', 10), variable=fun_var, value=4, command=print_selection)rb4.place(x=15, y=120)rb5 = tk.Radiobutton(window, text='狗粮队员', font=('微软雅黑', 10), variable=fun_var, value=5, command=print_selection)rb5.place(x=15, y=150)# Radiobutton # 开始按钮 start_mission()中定义了每一个功能所要执行的函数，注意的是，独立功能需要放在一个线程中执行，不然界面会被阻塞卡死 全局变量is_start用来控制功能的执行与停止 click()函数用来改变按钮显示以及锁定功能选择 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# button start#rb_list = [rb1, rb2, rb3, rb4, rb5]button_var = tk.StringVar()button_var.set('开始')is_click = Falsedef start_mission(): global is_start if fun_var.get() == 1: text.insert('end', strftime('%H:%M:%S', localtime()) + ' 开始执行寮突破\n') text.see('end') # 自动显示底部 window_size = get_window_info() if window_size: # 打开了阴阳师 window.geometry('240x480+%d+%d' % (window_size[0]-240, window_size[1])) is_start = True thread1 = threading.Thread(target=liao_tupo, args=(window_size,)) thread1.start() elif fun_var.get() == 2: text.insert('end', strftime('%H:%M:%S', localtime()) + ' 开始执行御灵、业原火\n') text.see('end') # 自动显示底部 window_size = get_window_info() if window_size: # 打开了阴阳师 window.geometry('240x480+%d+%d' % (window_size[0] - 240, window_size[1])) is_start = True thread2 = threading.Thread(target=yu_ling, args=(window_size,)) thread2.start() elif fun_var.get() == 3: text.insert('end', strftime('%H:%M:%S', localtime()) + ' 魂十队员功能未开发\n') text.see('end') # 自动显示底部 elif fun_var.get() == 4: text.insert('end', strftime('%H:%M:%S', localtime()) + ' 魂十队长功能未开发\n') text.see('end') # 自动显示底部 elif fun_var.get() == 5: text.insert('end', strftime('%H:%M:%S', localtime()) + ' 狗粮队员功能未开发\n') text.see('end') # 自动显示底部def stop_mission(): global is_start is_start = False text.insert('end', strftime('%H:%M:%S', localtime()) + ' 停止执行\n') text.see('end') # 自动显示底部def click(): global is_click if not is_click: is_click = True button_var.set('停止') label.config(text=fun_text + ' 已经开始') for rb in rb_list: # 将选项锁定 rb.config(state='disabled') button_adjust.config(state='disabled') start_mission() else: is_click = False button_var.set('开始') label.config(text=fun_text + ' 已经停止') for rb in rb_list: rb.config(state='active') button_adjust.config(state='active') stop_mission()button = tk.Button(window, textvariable=button_var, width=10, height=1, command=click)button.place(x=140, y=60)# button start# 文本显示1234import ScrolledTexttext = ScrolledText.ScrolledText(window, width=29, height=17) # 滚动输出文本框# text = tk.Text(window, width=29, height=17) # 输出文本框text.place(x=15, y=180) 注意的一点是，再每次输出文本的时候希望自动显示低端，这时需要在insert之后执行text.see(&#39;end&#39;) Pyinstaller打包1pyinstaller -F -w -i ./yaodao.ico ./tk_gui.py -F表示输出单文件exe -w表示不显示命令行 -i设置图标 更多参数设置详见这里 至此全部搞定，打开exe时记得右键管理员权限打开 Have Fun！]]></content>
      <categories>
        <category>不务正业</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git安装与基本使用]]></title>
    <url>%2F20170816%2Fgit-note%2F</url>
    <content type="text"><![CDATA[个人使用git问题记录 安装 Windows 下载Git 双击安装，一路next linux sudo apt-get install git 配置用户 12git config --global user.name 'zydarChen'git config --global user.email 'zydarChen@outlook.com' 生成ssh key 1ssh-keygen -t rsa -C 'zydarChen@outlook.com' 与GitHub、Coding连接 首先将id_rsa.pub内容复制进账户SSH Key 12ssh -T git@github.comssh -T git@git.coding.net Git基本使用参考： 廖雪峰Git教程 莫烦PythonGit版本管理 1234567891011121314git clone https://github.com/scut-githuber/os-util.git # clone repository到本地cd gitDirgit init # 初始化空的Git仓库repositorygit add filename # 将文件添加到repositorygit add -A # 添加所有文件到repositorygit commit -m &quot;wrote a readme file&quot; # 提交并写提交说明git status # 查看已修改文件git diff filename # 查看filename被修改的内容git log [--pretty=oneline]# 查看提交日志git reset --hard HEAD^ # 滚回上一版本git reset --hard 3628164 # 指定ID回滚或者重做，`3628164`是commit id，git自动补全git reflog # 查看命令历史，可以查看各个版本的commit idgit checkout -- name # 让这个文件回到最近一次git commit或git add时的状态。git reset HEAD filename # 将filename从暂存区退回工作区 工作区和暂存区 工作区:工作目录，如gitDir 版本库：工作区中的隐藏目录.git 第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区；第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。 流程图 初始在工作目录中添加’test.py’文件，文件处于’untracked’状态：红色的’??’ 12$ git status -s?? test.py add之后，文件处于’staged’状态：绿色的’A’ 123git add test.py$ git status -sA test.py commit之后，文件处于’unmodified’状态，working tree clean git log --oneline查看commit记录。查看使用reset可回到之前版本，git reset --hard HEAD^，此时工作目录中’test.py’文件将被删除。 add之后，修改’test.py’文件内容，文件处于’modified’状态：绿色的’A’——表示新增文件操作已添加至暂存区，可提交，红色的’M’——表示修改操作仍位于工作区，不可提交。此时执行commit只会提交新增文件操作。12345$ git status -sAM test.py$ git commit -m 'test.py'$ git status -sM test.py 撤销修改 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，在没有推送到远程库的前提下，git reset --hard HEAD^滚回上一版本。 误删与恢复git checkout -- test.txtgit checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 分支使用 git branch：查看分支 git branch &lt;name&gt;：创建分支 git checkout &lt;name&gt;：切换分支 git checkout -b &lt;name&gt;：创建并切换分支 git merge &lt;name&gt;：合并某分支到当前分支 git merge --no-ff -m &quot;merge with no-ff&quot; &lt;name&gt;:强制禁用Fast forward模式，不丢掉分支信息 git branch -d &lt;name&gt;：删除分支 bug分支当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场。 git stash:”储存”当前工作现场 git stash list:查看已储存现场 git stash apply:恢复现场 git stash pop：恢复现场并产出储存 JetBrains系列使用git小结 先在GitHub上建好repository，git clone 到本地 鼓励大量使用分支 通过创建bug分支的方式来修复bug 开发一个新feature，最好新建一个分支 .gitignore文件 #注释行 dir/忽略dir文件 *.zip忽略zip文件 ignore.md忽略单个文件 !不忽略 doc/*.md忽略doc/目录下所有的.md文件，但不包括其他子目录下的.md文件]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习基石》学习笔记#2]]></title>
    <url>%2F20170730%2FML-Foundations-2%2F</url>
    <content type="text"><![CDATA[handout slides Machine Learning Foundations 课程由台湾大学NTU林轩田老师开设，课程共16篇，包括四部分内容： When can machines learn? (illustrative + technical) Why can machines learn? (theoretical + technical) How can machines learn? (technical + practical) How can machines learn better? (practical + theoretical) 下面是Topic 1 Part 2——learning to answer yes/no 1. Perceptron Hypothesis Set 引入信用卡的例子，银行如何根据用户信息来决定是否给用户发放信用卡。 A Simple Hypothesis Set: the Perceptron 每个用户信息形成d维向量$\mathbf{x} = (x_1, x_2, …, x_d)$； 每个特征赋予不同的权值$w_i$，表示该特征对是否发放信用卡的影响； 所有特征的加权求和与一设定的阈值进行比较，大于阈值输出+1，即发放信用卡；小于阈值输出-1，不发放信用卡 令$x_0 = +1, w_0 = -threshold$，将阈值吸收进$\mathbf{w}^T$，则： $h(x) = sign(\sum_{i=0}^{d}w_ix_i) = sign(\mathbf{w}^T\mathbf{x})$ 2. Perceptron Learning Algorithm (PLA)在Perceptron中，hypothesis set由许多直线构成，PLA的目的就是在这些可能是无限的直线中，选择一条最好的直线，能将平面上所有的正类和负类完全分开，也就是找到最好的g，使$g \approx f$ 遍历所有可能的直线是不现实的，思路是采用“逐点修正” 基于一条直线，找到错误的点，并更新$w$，更新方法是： 如果错误点的$y = +1$，即正类误判为负类$\mathbf{w}_{(t)}\mathbf{x}_{n(t)} &lt; 0$，则表示$\mathbf{w}$与$\mathbf{x}$夹角大于90，修正方案是将角度变小，即$\mathbf{w} = \mathbf{w} + y\mathbf{x}, y=+1$； 如果错误点的$y = -1$，即负类误判为正类$\mathbf{w}_{(t)}\mathbf{x}_{n(t)} &gt; 0$，则表示$\mathbf{w}$与$\mathbf{x}$夹角小于90，修正方案是将角度变大，即$\mathbf{w} = \mathbf{w} + y\mathbf{x}, y=-1$； 如果数据本身是线性可分的，经过不断的迭代修正之后，所有的点都能正确分类 我们由$\mathbf{w}_{t+1} \leftarrow \mathbf{w}_t + y_n\mathbf{x}_n$，两边同时乘以$y_n\mathbf{x}_n$得到$y_n \mathbf{w}_{t+1}\mathbf{x}_n \leftarrow y_n \mathbf{w}_t\mathbf{x}_n + (y_n\mathbf{x}_n)^2$则有：$y_n \mathbf{w}_{t+1}\mathbf{x}_n \geqslant y_n \mathbf{w}_t\mathbf{x}_n$上式表明，随着迭代的进行，正确分类的样本逐渐变多the rule somewhat ‘tries to correct the mistake’. 3. Guarantee of PLA如果D不是线性可分的（linear separable），则PLA不会停止在线性可分的情况下，则存在$\mathbf{w}_f$使得$y_n = sing(\mathbf{w}_f^T \mathbf{x}_n)$成立$$\min_n \ y_n \mathbf{w}_f^T \mathbf{x}n &gt; 0 \Leftrightarrowy{n(t)} \mathbf{w}f^T \mathbf{x}{n(t)} \geqslant \min_n \ y_n \mathbf{w}_f^T \mathbf{x}_n &gt; 0$$ 也就是说，$\mathbf{w}_f^T \mathbf{w}_t$逐渐变大，内积越大似乎就表示两个向量越接近，其实不然，有可能是长度更接近了，而不是角度。所以，下面再证明长度关系。有错才更新，所以下面这个式子成立 可以看出，$\mathbf{w}_t$的增长被限制了，$\mathbf{w}_{t+1}$与$\mathbf{w}_t$向量的长度不会差别太大。 如果令初始权重$\mathbf{w}_0 = 0$，那么经过T次错误修正后，有如下结论：$$\frac{\mathbf{w}_f^T}{\left |\mathbf{w}_f \right |}\frac{\mathbf{w}_T}{\left |\mathbf{w}_T \right |} \geqslant \sqrt{T}\cdot constant$$总结： 线性可分：$\mathbf{w}_f^T$与$\mathbf{w}_t$接近 逐点纠错：$\mathbf{w}_t$长度缓慢成长 最终，PLA会停下来 4. Non-Separable Data 对于非线性的问题，PLA不会停止。 找到的“线”犯的错误最少 事实证明，上面的解是NP-hard问题，难以求解。 Packet Algorithm是对PLA的变形 “贪心”：把最好的“线”抓在手上modify PLA algorithm (black lines) by keeping best weights in pocket 一般情况下，Pocket Algorithm要比PLA速度慢一些。 附录：PLA收敛性证明]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习基石》学习笔记#1]]></title>
    <url>%2F20170727%2FML-Foundations-1%2F</url>
    <content type="text"><![CDATA[handout slides Machine Learning Foundations 课程由台湾大学NTU林轩田老师开设，课程共16篇，包括四部分内容： When can machines learn? (illustrative + technical) Why can machines learn? (theoretical + technical) How can machines learn? (technical + practical) How can machines learn better? (practical + theoretical) 下面是Topic 1 Part 1——the learning problem 0. Course Introductionfoundation oriented and story-like 1. What is Machine Learning learning VS Machine Learning learning: observations -&gt; learning -&gt; skill Machine Learning: data -&gt; ML -&gt; skill skill: import some performace measure, 提高某一性能 使用ML的三个关键条件： 事物本身存在某种规律 难以通过简单编程解决 有数据可供使用 2. Applications of Machine LearningML is everywhere食、衣、住、行、育、乐 3. Components of Machine Learning 基本术语 hypothesis g VS target f f是目标函数，反映问题的真实规律，但f一般是未知的； g是通过算法A得出的假设函数，我们希望g尽可能与f接近 hypothesis set H 假设集，一般一个问题对应了多个假设，这些假设形成假设集H，从H中找出最佳的g。 ML流程图 训练数据D满足未知的目标函数f 机器学习的过程，就是根据先验知识选择模型，该模型对应的hypothesis set（用H表示），H中包含了许多不同的hypothesis，通过演算法A，在训练样本D上进行训练，选择出一个最好的hypothesis，对应的函数表达式g就是我们最终要求的。 Machine Learning: use data to compute hypothesis g that approximates target f A takes D and H to get g 4. Machine Learning and Other Fields与ML相关的领域： Data Mining: use (huge) data to find property that is interestingdifficult to distinguish ML and DM in reality Artificial Intelligence: compute something that shows intelligent behaviorML is one possible route to realize AI Statistics: use data to make inference about an unknown processmany useful tools for ML]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GoogleML系列视频笔记]]></title>
    <url>%2F20170726%2FGoogleML-note%2F</url>
    <content type="text"><![CDATA[GoogleML系列目前共七个短视频，内容通俗易懂 Lesson1 Hello World123456from sklearn import treefeatures = [[140, 1], [130, 1], [150, 0], [170, 0]]labels = [1, 1, 1, 1]clf = tree.DecisionTreeClassifier()clf = clf.fit(features, labels)print clf.predict([150, 0]) Lesson 2 Visualizing a Decision TreeWhy decision Tree easy to read and understand Iris(Wiki) 经典的ML问题，花类型识别 四个features，三个labels 直接从sklearn导入 1234567# 查看数据from sklearn.datasets import load_irisiris = load_iris()print iris.feature_namesprint iris.target_namesprint iris.data[0]print iris.target[0] [&apos;sepal length (cm)&apos;, &apos;sepal width (cm)&apos;, &apos;petal length (cm)&apos;, &apos;petal width (cm)&apos;] [&apos;setosa&apos; &apos;versicolor&apos; &apos;virginica&apos;] [ 5.1 3.5 1.4 0.2] 0 123456789101112131415161718192021# 建立决策树分类器import numpy as npfrom sklearn.datasets import load_irisfrom sklearn import treeiris = load_iris()# 测试集索引，每个类取一个example作为测试集test_idx = [0, 50, 100]# training datatrain_target = np.delete(iris.target, test_idx)train_data = np.delete(iris.data, test_idx, axis=0)# testing datatest_target = iris.target[test_idx]test_data = iris.data[test_idx]clf = tree.DecisionTreeClassifier()clf.fit(train_data, train_target)print test_targetprint clf.predict(test_data) [0 1 2] [0 1 2] Visualize 可视化使用pydot pip install pydotplus conda install graphviz GraphViz’s executables not found 下载graphviz 安装，记下安装路径，如C:\Program Files (x86)\Graphviz2.38\bin 将路径添加到系统环境变量 重启IDE 123456789from IPython.display import Imageimport pydotplusdot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(dot_data)Image(graph.create_png(), width=500, height=500) Lesson 3 What Makes a Good Feature 多个feature能更好的训练模型 重复特征应当删除，否则分类器会多次使用相同特征，导致该特征被强调 feature分布越均匀，该feature对分类的作用越弱 feature应相互独立 feature应预处理，如经纬度信息经过转化可以形成距离等 总结 informative independent simple 123456789101112import numpy as npimport matplotlib.pyplot as plt# 构造500只greyhounds和500只labsgreyhounds = 500labs = 500grey_height = 28 + 4 * np.random.randn(greyhounds)lab_height = 24 + 4 * np.random.randn(labs)plt.hist([grey_height, lab_height], stacked=True, color=['r', 'b'])plt.show() Lesson 4 Let’s Write a Pipeline 划分训练集跟测试集，在训练集上训练，测试集上验证 调用sklearn.cross_validation.train_test_split切分数据集 本质上，是学习feature到label，从输入到输出的函数 神经网络演示playground 12345678910111213141516171819202122232425# import a datasetfrom sklearn import datasetsiris = datasets.load_iris()X = iris.datay = iris.target# splitfrom sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)from sklearn import treefrom sklearn.neighbors import KNeighborsClassifiertree_clf = tree.DecisionTreeClassifier()kn_clf = KNeighborsClassifier()tree_clf.fit(X_train, y_train)kn_clf.fit(X_train, y_train)tree_pred = tree_clf.predict(X_test)kn_pred = kn_clf.predict(X_test)from sklearn.metrics import accuracy_scoreprint 'tree_clf accuracy:', accuracy_score(y_test, tree_pred)print 'kn_clf accuracy:', accuracy_score(y_test, kn_pred) tree_clf accuracy: 0.946666666667 kn_clf accuracy: 0.986666666667 Lesson 5 Writing Our First Classifier简单的随机分类器12345678910111213141516171819202122232425262728293031import randomclass random_clf(): def fit(self, X_train, y_train):# pass self.X_train = X_train self.y_train = y_train def predict(self, X_test):# pass predictions = [] for row in X_test: label = random.choice(self.y_train) predictions.append(label) return predictions# import a datasetfrom sklearn import datasetsiris = datasets.load_iris()X = iris.datay = iris.target# splitfrom sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)clf = random_clf()clf.fit(X_train, y_train)clf_pred = clf.predict(X_test)from sklearn.metrics import accuracy_scoreprint 'accuracy:', accuracy_score(y_test, clf_pred) accuracy: 0.36 KNN (K-Nearest Neighbour) 考虑测试点的近邻K个点，K个点中，属于某一类最多，则该点属于该类 距离公式，平方和开方 注意： 实现时先确定接口(fit, predict) 对每个接口实现时先确定输入输出 K = 1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from scipy.spatial import distance# 计算a, b之间的距离def euc(a,b): return distance.euclidean(a, b)class knn_clf(): def fit(self, X_train, y_train):# pass self.X_train = X_train self.y_train = y_train def predict(self, X_test):# pass predictions = [] for row in X_test: label = self.closest(row) predictions.append(label) return predictions def closest(self, row): best_dist = euc(row, self.X_train[0]) best_index = 0 for i in range(1, len(self.X_train)): dist = euc(row, self.X_train[i]) if dist &lt; best_dist: best_dist = dist best_index = i return self.y_train[best_index]# import a datasetfrom sklearn import datasetsiris = datasets.load_iris()X = iris.datay = iris.target# splitfrom sklearn.cross_validation import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)clf = knn_clf()clf.fit(X_train, y_train)clf_pred = clf.predict(X_test)from sklearn.metrics import accuracy_scoreprint 'accuracy:', accuracy_score(y_test, clf_pred) accuracy: 0.986666666667 Lesson 6 Train an Image Classifier with TensorFlow for PoetsNo feature engineering needed!!!数据 五种花图片 218MB 如果你想要用其他的图片类型，你只需要创建一个新的文件夹，放入对应类型的100张以上的图片 Diversity and quantity Diversity：样本多样性越多，对新事物的预测能力越强 Quantity：样本数量越多，分类器越强大 以下代码在Linux下执行 12345678910111213141516171819202122from sklearn import datasets, cross_validation# tensorflow 1.1.0import tensorflow as tf# load datesetstf.logging.set_verbosity(tf.logging.ERROR) # 忽略其他日志信息iris = datasets.load_iris()X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.2)# Construct DNN# Specify that all features have real-value datafeature_columns = [tf.contrib.layers.real_valued_column('', dimension=4)]# Build 3 layer DNN with 10, 20, 10 units respectively.classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, # 指定数据特征，维数4 hidden_units=[10, 20, 10], # Three hidden layers, containing 10, 20, and 10 neurons, respectively. n_classes=3, # 三类 model_dir='/tmp/iris_model')classifier.fit(x=X_train, y=y_train, steps=1000)score = classifier.evaluate(x=X_test, y=y_test, steps=1)['accuracy']print('\nTest Accuracy: &#123;0:f&#125;\n'.format(score)) Test Accuracy: 1.000000 Lesson 7 Classifying Handwritten Digits with TF.Learnmnist问题 the Hello World of computer vision 训练集55,000，测试集10,000，每张图片处理成28*28的二维矩阵，784 features 十分类问题 123456import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimport tensorflow as tflearn = tf.contrib.learntf.logging.set_verbosity(tf.logging.ERROR) Import the dataset12345678910mnist = learn.datasets.load_dataset('mnist')data = mnist.train.imageslabels = np.asarray(mnist.train.labels, dtype=np.int32)test_data = mnist.test.imagestest_labels = np.asarray(mnist.test.labels, dtype=np.int32)# 减少数据max_examples = 10000data = data[:max_examples]labels = labels[:max_examples] Extracting MNIST-data/train-images-idx3-ubyte.gz Extracting MNIST-data/train-labels-idx1-ubyte.gz Extracting MNIST-data/t10k-images-idx3-ubyte.gz Extracting MNIST-data/t10k-labels-idx1-ubyte.gz 显示1234567def display(i): img = test_data[i] plt.title('Example %d. Label: %d' % (i, test_labels[i])) plt.imshow(img.reshape((28,28)), cmap=plt.cm.gray_r)display(0)print 'number of features is', len(data[0]) number of features is 784 fit a Linear Classifier123456feature_columns = learn.infer_real_valued_columns_from_input(data)classifier = learn.LinearClassifier(feature_columns=feature_columns, n_classes=10)classifier.fit(data, labels, batch_size=100, steps=1000)classifier.evaluate(test_data, test_labels)print classifier.evaluate(test_data, test_labels)['accuracy'] 0.9137 Visualize learned weights12345678910weights = classifier.weights_f, axes = plt.subplots(2, 5, figsize=(10,4))axes = axes.reshape(-1)for i in range(len(axes)): a = axes[i] a.imshow(weights.T[i].reshape(28, 28), cmap=plt.cm.seismic) a.set_title(i) a.set_xticks(()) # ticks be gone a.set_yticks(())plt.show() --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-32-13532f014713&gt; in &lt;module&gt;() ----&gt; 1 weights = classifier.weights_ 2 f, axes = plt.subplots(2, 5, figsize=(10,4)) 3 axes = axes.reshape(-1) 4 for i in range(len(axes)): 5 a = axes[i] AttributeError: &apos;LinearClassifier&apos; object has no attribute &apos;weights_&apos; 附录代码分析 From ahangchen代码分析 下载数据集 1mnist = learn.datasets.load_dataset('mnist') 恩，就是这么简单，一行代码下载解压mnist数据，每个img已经灰度化成长784的数组，每个label已经one-hot成长度10的数组 numpy读取图像到内存，用于后续操作，包括训练集（只取前10000个）和验证集 1234567data = mnist.train.imageslabels = np.asarray(mnist.train.labels, dtype=np.int32)test_data = mnist.test.imagestest_labels = np.asarray(mnist.test.labels, dtype=np.int32)max_examples = 10000data = data[:max_examples]labels = labels[:max_examples] 可视化图像 12345def display(i): img = test_data[i] plt.title('Example %d. Label: %d' % (i, test_labels[i])) plt.imshow(img.reshape((28, 28)), cmap=plt.cm.gray_r) plt.show() 用matplotlib展示灰度图 训练分类器 提取特征（这里每个图的特征就是784个像素值） 1feature_columns = learn.infer_real_valued_columns_from_input(data) 创建线性分类器并训练 12classifier = learn.LinearClassifier(feature_columns=feature_columns, n_classes=10)classifier.fit(data, labels, batch_size=100, steps=1000) 注意要制定n_classes为labels的数量 分类器实际上是在根据每个feature判断每个label的可能性， 不同的feature有的重要，有的不重要，所以需要设置不同的权重 一开始权重都是随机的，在fit的过程中，实际上就是在调整权重 最后可能性最高的label就会作为预测输出 传入测试集，预测，评估分类效果 12result = classifier.evaluate(test_data, test_labels)print result["accuracy"] 速度非常快，而且准确率达到91.4% 可以只预测某张图，并查看预测是否跟实际图形一致 123456# here's one it gets rightprint ("Predicted %d, Label: %d" % (classifier.predict(test_data[0]), test_labels[0]))display(0)# and one it gets wrongprint ("Predicted %d, Label: %d" % (classifier.predict(test_data[8]), test_labels[8]))display(8) 可视化权重以了解分类器的工作原理 12weights = classifier.weights_a.imshow(weights.T[i].reshape(28, 28), cmap=plt.cm.seismic) 这里展示了8个张图中，每个像素点（也就是feature）的weights， 红色表示正的权重，蓝色表示负的权重 作用越大的像素，它的颜色越深，也就是权重越大 所以权重中红色部分几乎展示了正确的数字]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark日常填坑]]></title>
    <url>%2F20170717%2Fdebug-spark%2F</url>
    <content type="text"><![CDATA[spark基础使用 集群spark使用jupyter notebook jupyter notebook安装 ImportError: No module named pyspark 原因是没有添加环境变量1234# spark 1.6.0export SPARK_HOME=/usr/local/sparkexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip# 注意py4j的版本对应 first try 123456from pyspark import SparkContext, SparkConfconf = SparkConf().setAppName('YOURNAME').setMaster('spark://mu01:7077').set('spark.executor.memory', '4G').set('spark.cores.max', '80')sc = SparkContext(conf=conf)data = [1, 2, 3, 4, 5]distData = sc.parallelize(data)print distData.first()]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写高质量代码]]></title>
    <url>%2F20170714%2F91-Suggestions%2F</url>
    <content type="text"><![CDATA[Writing Solid Python Code–91 Suggestions to Improve Your Python Program 《改善Python程序的91个建议》读书笔记 Pythonic编程 第1章 引论 Sug 1: Pythonic，充分体现Python自身特色的代码风格。 Sug 1: 实现快速排序函数： 12345678910111213141516def quicksort(array): less = [] greater = [] if len(array) &lt;= 1: return array pivot = array.pop() for x in array: if x &lt;= pivot: less.append(x) else: greater.append(x) return quicksort(less) + [pivot] + quicksort(greater) Sug 1: Pythonic最推荐的字符串格式化方法 1print '&#123;greet&#125; from &#123;language&#125;'.format(greet = 'Hello World', language = 'Python') Sug 2: 避免劣化代码 避免只用大小写来区分不同的对象 避免使用容易引起混淆的名称 不要害怕过长的变量名 总的来说，变量名的命名应更具有实际意义 Sug 2: PEP8是一篇关于Python编码风格的指南 123pip install -U pep8pep8 --first test.pypep8 --show-source --show-pep8 test.py Sug 3: 三元操作符?等价于X if C else Y Sug 3: switch...case语句实现：if...elif...elif...else或者使用跳转表实现 12345678def f(n): return &#123; 0: 'You typed zero.\n', 1: 'You are in top.\n', 2: 'n is an even number.\n' &#125;.get(n, 'Only single-digit number are allowed.\n')# dict.get(key, default=None) Sug 4: 函数注释实例 123456789def funcName(parameter1, parameter2): """Describe what this function does. Args: parameter1:parameter type, what is this parameter used for. parameter2:parameter type, what is this parameter used for. Returns: return type, return value """ function body Sug 6: if、elif、while、for等循环语句尽量不要嵌套过深，最好能控制在3层以内。 Sug 6: 函数参数设计应该考虑向下兼容 123def readfile(filename) # 第一个版本def readfile(filename, logger) # 第二个版本，不向下兼容def readfile(filename, logger = logger.info) # 第二个版本，向下兼容 Sug 7: 将常量集中到一个文件 12345678910111213141516171819202122# coding:utf-8#class _const: class ConstError(TypeError): pass class ConstCaseError(ConstError): pass def __setattr__(self, name, value): if name in self.__dict__: raise self.ConstError("can't change const %s" % name) if not name.isupper(): raise self.ConstCaseError('const name "%s" is not all uppercase' % name) self.__dict__[name] = value ## import sys# sys.modules[__name__] = _const()const = _const()const.PI = 3.14# 在另一个.py文件中引用from const import constprint const.PI 第2章 编程惯用法 Sug 8: assert用来捕捉用户所定义的约束，而不是用来捕捉程序本身错误的 1assert x == y, 'not equals' Sug 9: 充分利用Lazy evaluation的特性 if x and y, x为false时不计算y if x or y, x为true时不计算y Sug 12: 推荐使用isinstance()检查类型，而不是type() 1isinstance('a',(str, unicode)) Sug 15: 使用enumerate()获取序列迭代的索引和值 12345678910111213for i,e in enumerate(list):# 函数原型def enumerate(sequence, start = 0): n = start for elem in sequence: yield n, elem n += 1# 反序def myenumerate(sequence): n = -1 for elem in reversed(sequence): yield len(sequence) + n, elem n = n - 1 Sug 16: is表示的是对象标识符，而==表示的意思是相等。 Sug 17: decode()将其他编码对应的字符串解码成unicode，而encode()将unicode编码转换为另一种编码。 Sug 17: 源文件编码说明# coding = utf-8 第3章 基础语法 Sug 19: import的使用 一般情况下尽量优先使用import a.B 有节制地使用from a import B 尽量避免使用from a import * Sug 22: 使用with自动关闭资源 12with open('test.text', 'w') as f:f.write('test') Sug 23: 使用else子句简化循环，当循环自然终结时else从句会被执行一次。 Sug 24: 异常处理try-except-else-finally 1234567891011121314try: &lt;statements&gt;except &lt;name1&gt;: &lt;statements&gt; # 当try中发生name1的异常时处理except (name2, name3): &lt;statements&gt; # 当try中发生name2或name3中的某一个异常时处理except &lt;name4&gt; as &lt;data&gt;: &lt;statements&gt; # 当try中发生name4的异常时处理，并获取对应实例except: &lt;statements&gt; # 其他异常发生时处理else: &lt;statements&gt; # 没有异常发生时处理finally: &lt;statements&gt; # 不管有没有异常发生都会执行 Sug 25: 不推荐在finally中使用return语句进行返回 如果finally语句中产生了新的异常或者执行了return或者break语句，那么临时保存的异常信息将会被丢失 在执行try语句块的return之前，如果finally语句块中存在return会直接返回 Sug 26: 判断列表是否为空` 12if list1: do something Sug 27: 连接字符串应优先使用join而不是+ Sug 28: 格式化字符串转换类型 转换类型 解释 c 转换为单个字符，对于数字将转换该值对应的ASCII码 + 转化为字符串，对于非字符串对象，将默认调用str()函数进行转换 r 用repr()函数进行字符串转换 i d 转换为带符号的十进制数 u 转换为不带符号的十进制数 o 转化为不带符号的八进制数 x X 转化为不带符号的十六进制 e E 表示为科学记数法表示的浮点数 f F 转成浮点数（小数部分自然截断） g G 如果指数大于-4或者小于精度值则和e/E相同，其他情况与f/F相同 Sug 30: 列表解析[expr for iter_item in iterable if cond_expr] Sug 31: 函数传参既不是传值也不是传引用，应该是传对象。根绝对象是否可变进行区分 Sug 33: 慎用变长参数 *args: 用于接受一个包装为元组形式的参数列表来传递非关键参数 **kwags: 接受字典形式的关键字参数列表 Sug 34: str()主要面向用户，其目的是可读性；repr面向的是python解释器 第4章 库 Sug 36: Python遇到未闭合的小括号时会自动将多行代码拼接为一行，并把相邻的两个字符串字面量拼接在一起 1234&gt;&gt;&gt;s = ('this is ' 'a long sentence')&gt;&gt;&gt;s'this is a long sentence' Sug 36: 判断一个变量s是不是字符串应使用isinstance(s, basestring)，basestring是str和unicode的基类 Sug 36: split()先去除字符串两端的空白字符，然后以任意长度的空白字符串作为界定符分切字符串；而split(&#39;&#39;)直接以空格作为界定符 Sug 37: sorted()函数返回一个排序后的列表，原有列表保持不变；而sort()函数会直接修改原有列表，函数返回None，因为不需要复制原有列表，效率相对较高。 12345678from operator import itemgettergameresult = [ ['Bob', 95.00, 'A'], ['Alan', 86.00, 'C'], ['Mandy', 82.50, 'A'], ['Rob', 86.00, 'E'] ]sorted(gameresult, key = itemgetter(2, 1))#output[['Mandy', 82.5, 'A'],['Bob', 95.0, 'A'],['Alan', 86.0, 'C'],['Rob', 86.0, 'E']] Sug 38: 浅拷贝和深拷贝 浅拷贝(shallow copy): 构造一个新的复合对象并将从原对象中发现的引用插入该对象中。 深拷贝(deep copy): 构造一个新的复合对象，遇到引用会继续递归拷贝其所指向的具体内容。 Sug 42: 使用pandas处理大型CSV文件 Sug 44 : 序列化，把内存中的数据结构在不丢失其身份和类型信息的情况下转成对象的文本或二进制表示的过程。从效率上，json &gt; pickle &gt; cPickle 第8章 性能剖析与优化 Sug 79: 让正确的程序更快比让快速的程序正确容易得多 Sug 81: 80/20法则，20%的代码的运行时间占用了80%的总运行时间 1234567# 略if __name__ == '__main__': import cProfile cProfile.run('foo()', 'prof.txt') import pstats p = pstats.Stats('prof.txt') p.sort_stats('time').print_stats() Sug 83: 时间复杂度比较O(1) &lt; O(log* n) &lt; O(n) &lt; O(n log n) &lt; O(n2) &lt; O(cn) &lt; O(n!) &lt; O(nn)]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter与Pycharm远程开发]]></title>
    <url>%2F20170711%2Fjupyter-and-pycharm-for-remote%2F</url>
    <content type="text"><![CDATA[本地代码开发，远程服务器跑程序，想想是不是美滋滋呢~情景是这样的：你并没有服务器的root权限，你只是Ubuntu服务器上小小的user 登陆服务器openVPN 接入内网Xshell 登陆服务器 方法一：直接用ssh + ip 登陆，如ssh 192.168.0.100 方法二：新建会话，“连接”中配置名称、协议、主机、端口，“用户身份认证”中配置用户名、密码，之后双击会话即可登陆服务器。 jupyter notebook安装登陆系统之后很开心地pip install jupyter，然后开始各种权限不够，怎么办？那我想办法直接安装在用户目录行了吧，于是乎… pip怎么安装到用户目录 好像调用地还是系统的pip，权限又不够了。好吧，装一个自己的pip12wget https://bootstrap.pypa.io/get-pip.pypython get-pip.py 好像这次调用了系统的Python，于是乎…算球，先给本user装一套python，嗯，又是一番折腾，结论就是：强烈推荐直接安装Anaconda123456# Linux64位 Python-2.7.13# 其他版本自行上官网wget https://repo.continuum.io/archive/Anaconda2-4.3.1-Linux-x86_64.shchmod +x Anaconda2-4.3.1-Linux-x86_64.sh # 添加执行权限./Anaconda2-4.3.1-Linux-x86_64.sh # 安装# 接下来根据提示输入yes即可 因为Anaconda内置了jupyter，任务结束了输入jupyter notebook即可打开 配置首先生成notebook配置文件：jupyter notebook --generate-config，一般路径是~/.jupyter/jupyter_notebook_config.py，vim直接打开，配置内容如下： 12345c.NotebookApp.ip = &apos;*&apos;c.NotebookApp.notebook_dir = u&apos;/the/path/of/jupyternotebook&apos; # 配置notebook目录c.NotebookApp.open_browser = False # 默认不打开浏览器页面c.NotebookApp.password = u&apos;&apos; # notebook密码，生成方式在配置文件中有c.NotebookApp.port = 1717 # 配置端口，避免跟别人冲突嘛 万事俱备，jupyter notebook开启notebook等等，说好的本地开发呢？请打开本地浏览器，地址栏输入ip + 端口号，如192.168.0.100：1717，输入密码，log in，大功告成！ Pycharm使用PyCharm进行远程开发和调试上面的博文中已经有了图文并茂的详细说明，这里只做简单摘要总结，方便自己。 同步 Tools -&gt; Deployment -&gt; Configuration 点击+ Type选择SFTP，点击OK 第一个选项卡Connection填写主机、端口、根目录、用户名、密码 第二个选项卡Mappings填写本地项目地址，远程项目地址，第三行可留空 第三个选项卡Excluded Paths添加忽略路径 点击OK搞定，在Tools -&gt; Deployment中即可上传、下载、同步、浏览服务器文件 远程调试 选择File -&gt; Settings，选择Project -&gt; Project Interpreter，然后在右边，点击那个小齿轮设置，选择Add Remote 选择Deployment configuration，点击create，填入服务器python解释器路径 OK大功告成]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello Blog]]></title>
    <url>%2F20170707%2Fhello-blog%2F</url>
    <content type="text"><![CDATA[Hexo搭建博客问题记录2018.01.16更新 clean-blog主题移动端效果一般，现转NexT(使用文档，Github) 参考链接： Hexo(1)-github＋hexo 建立你的第一个博客 Hexo(2)-部署博客及更新博文 初入 Git安装 Node.js安装 Hexo安装npm install hexo-cli -g Hexo初始化 123hexo init blog # blog为文件夹名hexo ghexo s # 本地预览 修改根配置文件“\blog\_config.yml” NexT主题下插件安装 12npm install hexo-generator-searchdb --savenpm install hexo-symbols-count-time --save 常用操作 1234567hexo new 'title' # 新建博文hexo new page 'page_name' # 新建页面hexo cleanhexo generate # 生成静态页面，hexo ghexo server # 本地预览http://localhost:4000，hexo shexo deploy # 发布到GitHub，hexo dhexo d -g # 生成静态页面并发布 基础篇Q1：使用https与GitHub连接失败改用ssh1234deploy: type: git repository: https://github.com/username/username.github.io.git branch: master Q2：更换Clean Blog主题1git clone https://github.com/klugjo/hexo-theme-clean-blog.git themes/clean-blog 修改_config.yml1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: clean-blog Q3：如何将某一个标签作为一个页面将/tags/该标签作为页面链接 Q4：Clean Blog添加关于页面\themes\clean-blog\layout文件夹下创建about.ejs123456789101112131415161718192021222324252627282930&lt;!-- Page Header --&gt;&lt;!-- Set your background image for this header in your post front-matter: cover --&gt;&lt;% var cover = page.cover || theme.index_cover;%&gt;&lt;header class=&quot;intro-header&quot; style=&quot;background-image: url(&apos;&lt;%- cover %&gt;&apos;)&quot;&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1&quot;&gt; &lt;div class=&quot;site-heading&quot;&gt; &lt;h1&gt;&lt;%- page.title %&gt;&lt;/h1&gt; &lt;hr class=&quot;small&quot;&gt; &lt;span class=&quot;subheading&quot;&gt;&lt;%- page.subtitle %&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/header&gt;&lt;!-- Post Content --&gt;&lt;article&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;!-- Post Main Content --&gt; &lt;div class=&quot;col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1&quot;&gt; &lt;%- page.content %&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/article&gt; 生成about页面，修改\source\about\index.md，添加layout: about Q5：删除底部信息在\themes\clean-blog\layout\_partial\footer.ejs中修改 Q6：指定404页面将404.html文件放在\themes\clean-blog\source或\source中 自定义404页面1234567891011&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;404&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//qzonestyle.gtimg.cn/qzone/hybrid/app/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;http://yoursite.com/yourPage.html&quot; homePageName=&quot;回到我的主页&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 注意：如果没有绑定域名的话是自定义404页面，如腾讯公益404无法使用 hexo new page 404生成404页面，修改\source\404\index.md，hexo g生成静态页面，将生成的静态页面放在\themes\clean-blog\source或\source下，删除\source\404 Q6：显示网站缩略标志修改\themes\clean-blog\_config.yml12# set your own faviconfavicon: /img/favicon.jpg Q7：添加评论修改\themes\clean-blog\_config.yml123comments: # Disqus comments disqus_shortname: zydarChen 正常显示评论需科学上网 补充：添加其他评论，如来比力 来比力官网注册账号，根据提示获取安装代码 将代码复制到\themes\clean-blog\layout\_partial\comments.ejs即可 Q8：修改锚链接样式修改\themes\clean-blog\source\css\base.styl1234p, li a color brand-primary text-decoration none Q9：底部显示个人账号图标以及链接修改\themes\clean-blog\layout\_partial\footer.ejs，并在\themes\clean-blog\_config.yml下添加相应内容即可12# Social Accountswechat: /img/wechat.jpg 注意，由于之前Q8改过锚链接样式，而图标外圈颜色在标签p/li/a中，颜色会被修改为蓝色，直接将标签改为p/li/c，并在\themes\clean-blog\source\css\base.styl中添加相应的标签样式12345678910&lt;% if (theme.wechat) &#123; %&gt; &lt;li&gt; &lt;a href=&quot;&lt;%- theme.wechat%&gt;&quot; target=&quot;_blank&quot;&gt; &lt;span class=&quot;fa-stack fa-lg&quot;&gt; &lt;c class=&quot;fa fa-circle fa-stack-2x&quot;&gt;&lt;/c&gt; &lt;i class=&quot;fa fa-wechat fa-stack-1x fa-inverse&quot;&gt;&lt;/i&gt; &lt;/span&gt; &lt;/a&gt; &lt;/li&gt;&lt;% &#125; %&gt; Q10：页面图片宽度不一Clean Blog采用的是自适应，当内容不一致是出现这个问题。尽量保持内容模块数一致。Clean Blog主题的主页相较其他page页多出subtilte，补充即可。1234567&lt;div class=&quot;col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1&quot;&gt; &lt;div class=&quot;site-heading&quot;&gt; &lt;h1&gt;&lt;%- page.title %&gt;&lt;/h1&gt; &lt;hr class=&quot;small&quot;&gt; &lt;span class=&quot;subheading&quot;&gt;zydarChen&lt;/span&gt; &lt;/div&gt;&lt;/div&gt; 进阶篇Q11：部分文章不显示在主页如何控制文章是否在主页显示？比如，某些还只是处于草稿状态，不想放在主页，但可以归于某一标签或分类，在该分类中查看草稿。求告知。 Q12：托管到Coding，简单几步实现转移 注册Coding，最好使用注册GitHub的邮箱 ssh对接 如果coding邮箱与GitHub邮箱不同，将coding改绑到GitHub邮箱（懒人做法） 打开C:\Users\zydar\.ssh\id_rsa.pub，zydar是你电脑的用户名，复制文件内容 在coding个人主页，账户-SSH公钥，将id_rsa.pub文件内容复制进去 执行ssh -T git@git.coding.net，获得提示Hello zydar You&#39;ve connected to Coding.net by SSH successfully! 在coding上新建项目，项目名称为用户名 配置hexo，修改_config.yml 123456deploy: type: git repository: github: https://github.com/zydarChen/zydarChen.github.io.git coding: https://coding.net/zydarChen/zydarChen.git branch: master 在source/需要创建一个空白文件，至于原因，是因为coding.net需要这个文件来作为以静态文件部署的标志。就是说看到这个Staticfile就知道按照静态文件来发布。 12cd source/touch Staticfile #名字必须是Staticfile hexo g -d之后，会弹出页面填写coding.net账户密码 在coding.net进入项目，代码-Pages服务，开启服务即可 搞定，可以访问http://zydarChen.coding.me，查看你的博客了 Q13：将网站提交搜索引擎解决方案:Github(google提交)+Coding(百度提交)，自适应提交搜索引擎(绝招来啦！) Google网站验证链接 百度网站验证链接 确认收录方式：Google/百度搜索框输入site:yoursite.github.io 配置站点地图文件代码可以不添加，我添加反而出错 出现sitemap.xml文件中url为yoursite.com的情况，请检查_config.yml文件中的url:是否为网站地址 github禁止了百度爬虫，提交了百度也是不会访问的。百度验证时，通过文件验证出错也是这个原因。 如果同时托管到了coding.net，可以将百度sitemap的url改为coding的地址 进入\node_modules\hexo-generator-baidu-sitemap\baidusitemap.ejs 第三行&lt;% var url = config.url + config.root %&gt;改为&lt;% var url = config.coding_url + config.root %&gt; 修改站点_config.yml 12url: https://zydarchen.github.iocoding_url: http://zydarchen.coding.me # 添加行 hexo g之后能看到baidusitemap.xml已经自动修改 Q14：域名选路解析解决方案：Dnspod+Namesilo域名结合实现域名选路解析(精)我的解决方案： 万网上注册域名，目前已被阿里云收购 添加域名解析(我直接使用了万网提供的域名解析，也可以使用Dnspod等) Coding Pages配置，直接添加自定义域名www.zydarChen.top GitHub Pages配置，Hexo目录下建立CNAME文件，并将www.zydarChen.top写入，部署到服务器即可。在 Settings-GitHub Pages页面可看到部署成功。 按上述做法，当海外地址访问www.zydarChen.top时会解析到zydarchen.github.io，此时可能出现链接不安全/证书风险问题。这是GitHub Pages本身的问题，官方文档上写着“HTTPS is not supported for GitHub Pages using custom domains”解决方案：让个人域名下GithubPage完美支持https Q15：为页面添加阅读人数统计解决方案：Hexo统计post阅读次数我的实现： 注册LeanClound，创建Counter并记录APP ID跟APP Key 修改站点_config.yml，添加 12345# leancloudleancloud_visitors: enable: true app_id: UaWT************ app_key: U7TR*********** 修改\themes\clean-blog\layout\_partial\article-full.ejs，确定显示阅读量的位置 123456789101112&lt;span class=&quot;meta&quot;&gt; &lt;!-- Date and Author --&gt; &lt;% if(item.author) &#123; %&gt; Posted by &lt;%- item.author %&gt; on &lt;% &#125; %&gt; &lt;% if(item.date) &#123; %&gt; &lt;%= item.date.format(config.date_format) %&gt; &lt;% &#125; %&gt; &lt;!-- Lean Cloud --&gt; # 此处开始为添加部分 &lt;% if(config.leancloud_visitors.enable)&#123; %&gt; 阅读量 : &lt;span id=&quot;&lt;%= url_for(page.path) %&gt;&quot; class=&quot;leancloud_visitors&quot; data-flag-title=&quot;&lt;%- page.title %&gt;&quot;&gt;&lt;/span&gt; &lt;% &#125; %&gt;&lt;/span&gt; 修改\themes\clean-blog\layout\_partial\after-footer.ejs，添加 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!-- Lean Cloud --&gt;&lt;script src=&quot;//cdn1.lncld.net/static/js/2.5.0/av-min.js&quot;&gt;&lt;/script&gt;&lt;script&gt; var APP_ID = &apos;&lt;%- config.leancloud_visitors.app_id %&gt;&apos;; var APP_KEY = &apos;&lt;%- config.leancloud_visitors.app_key %&gt;&apos;; AV.init(&#123; appId: APP_ID, appKey: APP_KEY &#125;); // 显示次数 function showTime(Counter) &#123; var query = new AV.Query(&quot;Counter&quot;); if($(&quot;.leancloud_visitors&quot;).length &gt; 0)&#123; var url = $(&quot;.leancloud_visitors&quot;).attr(&apos;id&apos;).trim(); // where field query.equalTo(&quot;words&quot;, url); // count query.count().then(function (number) &#123; // There are number instances of MyClass where words equals url. $(document.getElementById(url)).text(number? number : &apos;--&apos;); &#125;, function (error) &#123; // error is an instance of AVError. &#125;); &#125; &#125; // 追加pv function addCount(Counter) &#123; var url = $(&quot;.leancloud_visitors&quot;).length &gt; 0 ? $(&quot;.leancloud_visitors&quot;).attr(&apos;id&apos;).trim() : &apos;www.zydarChen.top&apos;; # 你的网址 var Counter = AV.Object.extend(&quot;Counter&quot;); var query = new Counter; query.save(&#123; words: url &#125;).then(function (object) &#123; &#125;) &#125; $(function () &#123; var Counter = AV.Object.extend(&quot;Counter&quot;); addCount(Counter); showTime(Counter); &#125;);&lt;/script&gt; 已经搞定，但字体是斜体还是看着不舒服，修改\themes\clean-blog\source\css\base.styl，搜索italic，将相应描述删除，done Q16：字数统计WordCound 安装WordCound 1npm install hexo-wordcount --save 修改站点_config.yml，添加 1234# WordCountpost_wordcount: wordcount: true min2read: true 修改\themes\clean-blog\layout\_partial\article-full.ejs，在适当的位置添加以下代码（同Q14） 12345&lt;!-- WordCount --&gt;&lt;% if(config.post_wordcount.wordcount)&#123; %&gt; | 字数 : &lt;%= wordcount(page.content) %&gt;&lt;% &#125; %&gt;&lt;% if(config.post_wordcount.min2read)&#123; %&gt; | 阅读时长 : &lt;%= min2read(page.content) %&gt; min&lt;% &#125; %&gt; Q17：使用mathjax渲染latex如何处理Hexo和MathJax的兼容问题 安装插件 1npm install hexo-math --save 解决兼容问题 12npm uninstall hexo-renderer-marked –savenpm install hexo-renderer-kramed –save 测试，参考hexo-math MathJax Inline: 1Simple inline $a = b + c$. 效果：Simple inline $a = b + c$. MathJax Block: 1234$$\frac&#123;\partial u&#125;&#123;\partial t&#125;= h^2 \left( \frac&#123;\partial^2 u&#125;&#123;\partial x^2&#125; +\frac&#123;\partial^2 u&#125;&#123;\partial y^2&#125; +\frac&#123;\partial^2 u&#125;&#123;\partial z^2&#125;\right)$$ 效果：$$\frac{\partial u}{\partial t}= h^2 \left( \frac{\partial^2 u}{\partial x^2} +\frac{\partial^2 u}{\partial y^2} +\frac{\partial^2 u}{\partial z^2}\right)$$ 添加Mathjax开关How? Q18：添加折叠块功能 Hexo next博客添加折叠块功能添加折叠代码块 jQuery 实现内容折叠功能 使用 123&#123;% fold 提示语 %&#125;折叠内容&#123;% endfold %&#125; 搬运 themes/next/source/js/src/post-details.js # 主要实现点击显/隐代码 12345678// 设置折叠快$(document).ready(function()&#123; $(document).on('click', '.fold_hider', function()&#123; $('&gt;.fold', this.parentNode).slideToggle(); $('&gt;:first', this).toggleClass('open'); &#125;); $("div.fold").css("display","none");&#125;); themes/next/scripts/fold.js # 使用内建标签点击显/隐代码 123456function fold (args, content) &#123; var text = args[0]; if(!text) text = "点击显/隐"; return '&lt;div&gt;&lt;div class="fold_hider"&gt;&lt;div class="close hider_title"&gt;' + text + '&lt;/div&gt;&lt;/div&gt;&lt;div class="fold"&gt;\n' + hexo.render.renderSync(&#123;text: content, engine: 'markdown'&#125;) + '\n&lt;/div&gt;&lt;/div&gt;';&#125;hexo.extend.tag.register('fold', fold, &#123;ends: true&#125;); themes/next/scripts/tags.js # 修复代码块显示问题点击显/隐代码 12345678910111213141516171819const rEscapeContent = /&lt;escape(?:[^&gt;]*)&gt;([\s\S]*?)&lt;\/escape&gt;/g;const placeholder = '\uFFFD';const rPlaceholder = /(?:&lt;|&amp;lt;)\!--\uFFFD(\d+)--(?:&gt;|&amp;gt;)/g;const cache = [];function escapeContent(str) &#123; return '&lt;!--' + placeholder + (cache.push(str) - 1) + '--&gt;';&#125;hexo.extend.filter.register('before_post_render', function(data) &#123; data.content = data.content.replace(rEscapeContent, function(match, content) &#123; return escapeContent(content); &#125;); return data;&#125;);hexo.extend.filter.register('after_post_render', function(data) &#123; data.content = data.content.replace(rPlaceholder, function() &#123; return cache[arguments[1]]; &#125;); return data;&#125;); themes/next/source/css/_custom/custom.styl # 样式点击显/隐代码 12345678910.hider_title&#123; font-family: "Microsoft Yahei"; cursor: pointer;&#125;.close:after&#123; content: "▼";&#125;.open:after&#123; content: "▲";&#125;]]></content>
      <categories>
        <category>程序员</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码与解码]]></title>
    <url>%2F20161029%2Fencode_decode%2F</url>
    <content type="text"><![CDATA[【个人GitBook搬运】解除你对编码解码的困惑 字符串和编码-廖雪峰 字符编码 编码 语言 ASCII GB2312/GBK/GB18030 中文 Shift_JIS 日文 Euc-kr 韩文 使用unicode统一编码解决乱码问题 代价是，在英文上，unicode编码比ASCII编码多一倍存储 UTF-8编码把一个unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。 如果我们把所有现有的中、日、韩三国编码的非ASCII字符文本数据转换成UTF-8编码，则其大小都会变成原来的1.5倍。 一般情况下，记事本使用utf-8存储.txt文件，读取时转化为unicode编码显示；浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器 python编码 对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符 u&#39;中&#39;等价于u&#39;\u4e2d&#39;u&#39;A&#39;等价于u&#39;\u0041&#39; unicode转utf-8(采用utf-8编码) 12&gt;&gt;&gt; u'中文'.encode('utf-8')'\xe4\xb8\xad\xe6\x96\x87' str.encode(encoding=&#39;UTF-8&#39;,errors=&#39;strict&#39;)strict: 抛异常ignore: 忽略异常replace: 替换（可用于查错）xmlcharrefreplace: 用适当的XML字符引用替换(仅用于encode)backslashreplace: 用反斜杠转义序列(仅用于encode)通过 codecs.register_error() 注册的任何值 utf-8转unicode(解码成unicode) 12&gt;&gt;&gt; print '\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')中文 12str --- decode方法 ---&gt; unicodeunicode --- encode方法 ---&gt; str # -*- coding: utf-8 -*-由于Python源代码也是一个文本文件，所以，当你的源代码中包含中文的时候，在保存源代码时，就需要务必指定保存为UTF-8编码。当Python解释器读取源代码时，为了让它按UTF-8编码读取，我们通常在文件开头写上这两行： 12#!/usr/bin/env python# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。 sys模块设置默认编码可以省去很多麻烦，默认编码为ascii12345#coding:utf-8import sysreload(sys)sys.setdefaultencoding("utf-8")sys.getdefaultencoding() # 查看默认编码 print repr()通过print repr()可以查看python字符串的真实面貌 codecs 模块1234import codecsfp1 = codecs.open(filename,'w')fp2 = codecs.open(filename, 'r', 'utf-8')lines = fp2.readlins() raw_unicode_escape我们会遇到这样的情况：12&gt;&gt;&gt; print u'\xb5\xda\xd2\xbb\xbe\xed'.decode('gb2312')UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-5: ordinal not in range(128) 其实目的是想把字符串用gb2312解码：12&gt;&gt;&gt; print '\xb5\xda\xd2\xbb\xbe\xed'.decode('gb2312')第一卷 raw_unicode_escape可以帮我们去掉unicode前面的u12&gt;&gt;&gt; print u'\xb5\xda\xd2\xbb\xbe\xed'.encode('raw_unicode_escape').decode('gb2312')第一卷 附 Unicode和Python的中文处理 详解 python 中文编码与处理 来源：知乎 很久很久以前，有一群人，他们决定用8个可以开合的晶体管来组合成不同的状态，以表示世界上的万物。他们看到8个开关状态是好的，于是他们把这称为”字节“。再后来，他们又做了一些可以处理这些字节的机器，机器开动了，可以用字节来组合出很多状态，状态开始变来变去。他们看到这样是好的，于是它们就这机器称为”计算机“。 开始计算机只在美国用。八位的字节一共可以组合出256(2的8次方)种不同的状态。 他们把其中的编号从0开始的32种状态分别规定了特殊的用途，一但终端、打印机遇上约定好的这些字节被传过来时，就要做一些约定的动作。遇上0×10, 终端就换行，遇上0×07, 终端就向人们嘟嘟叫，例好遇上0x1b, 打印机就打印反白的字，或者终端就用彩色显示字母。他们看到这样很好，于是就把这些0×20以下的字节状态称为”控制码”。他们又把所有的空 格、标点符号、数字、大小写字母分别用连续的字节状态表示，一直编到了第127号，这样计算机就可以用不同字节来存储英语的文字了。大家看到这样，都感觉 很好，于是大家都把这个方案叫做ANSI 的”Ascii”编码（American Standard Code for Information Interchange，美国信息互换标准代码）。当时世界上所有的计算机都用同样的ASCII方案来保存英文文字。 后来，就像建造巴比伦塔一样，世界各地的都开始使用计算机，但是很多国家用的不是英文，他们的字母里有许多是ASCII里没有的，为了可以在计算机保存他们的文字，他们决定采用 127号之后的空位来表示这些新的字母、符号，还加入了很多画表格时需要用下到的横线、竖线、交叉等形状，一直把序号编到了最后一个状态255。从128 到255这一页的字符集被称”扩展字符集“。从此之后，贪婪的人类再没有新的状态可以用了，美帝国主义可能没有想到还有第三世界国家的人们也希望可以用到计算机吧！ 等中国人们得到计算机时，已经没有可以利用的字节状态来表示汉字，况且有6000多个常用汉字需要保存呢。但是这难不倒智慧的中国人民，我们不客气地把那些127号之后的奇异符号们直接取消掉, 规定：一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到 0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了。在这些编码里，我们还把数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的”全角”字符，而原来在127号以下的那些就叫”半角”字符了。 中国人民看到这样很不错，于是就把这种汉字方案叫做 “GB2312“。GB2312 是对 ASCII 的中文扩展。 但是中国的汉字太多了，我们很快就就发现有许多人的人名没有办法在这里打出来，特别是某些很会麻烦别人的国家领导人。于是我们不得不继续把 GB2312 没有用到的码位找出来老实不客气地用上。 后来还是不够用，于是干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK包括了GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。 后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK扩成了 GB18030。从此之后，中华民族的文化就可以在计算机时代中传承了。 中国的程序员们看到这一系列汉字编码的标准是好的，于是通称他们叫做 “DBCS“（Double Byte Charecter Set 双字节字符集）。在DBCS系列标准里，最大的特点是两字节长的汉字字符和一字节长的英文字符并存于同一套编码方案里，因此他们写的程序为了支持中文处理，必须要注意字串里的每一个字节的值，如果这个值是大于127的，那么就认为一个双字节字符集里的字符出现了。那时候凡是受过加持，会编程的计算机僧侣 们都要每天念下面这个咒语数百遍： “一个汉字算两个英文字符！一个汉字算两个英文字符……” 因为当时各个国家都像中国这样搞出一套自己的编码标准，结果互相之间谁也不懂谁的编码，谁也不支持别人的编码，连大陆和台湾这样只相隔了150海里，使用着同一种语言的兄弟地区，也分别采用了不同的 DBCS 编码方案——当时的中国人想让电脑显示汉字，就必须装上一个”汉字系统”，专门用来处理汉字的显示、输入的问题，但是那个台湾的愚昧封建人士写的算命程序就必须加装另一套支持 BIG5 编码的什么”倚天汉字系统”才可以用，装错了字符系统，显示就会乱了套！这怎么办？而且世界民族之林中还有那些一时用不上电脑的穷苦人民，他们的文字又怎么办？ 真是计算机的巴比伦塔命题啊！ 正在这时，大天使加百列及时出现了——一个叫 ISO （国际标谁化组织）的国际组织决定着手解决这个问题。他们采用的方法很简单：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号 的编码！他们打算叫它”Universal Multiple-Octet Coded Character Set”，简称 UCS, 俗称 “unicode“。unicode开始制订时，计算机的存储器容量极大地发展了，空间再也不成为问题了。于是 ISO 就直接规定必须用两个字节，也就是16位来统一表示所有的字符，对于ASCII里的那些“半角”字符，unicode包持其原编码不变，只是将其长度由原来的8位扩展为16位，而其他文化和语言的字符则全部重新统一编码。由于”半角”英文符号只需要用到低8位，所以其高8位永远是0，因此这种大气的方案在保存英文文本时会多浪费一倍的空间。 这时候，从旧社会里走过来的程序员开始发现一个奇怪的现象：他们的strlen函数靠不住了，一个汉字不再是相当于两个字符了，而是一个！是的，从unicode开始，无论是半角的英文字母，还是全角的汉字，它们都是统一的”一个字符“！同时，也都是统一的”两个字节“，请注意”字符”和”字节”两个术语的不同，“字节”是一个8位的物理存贮单元，而“字符”则是一个文化相关的符号。在unicode中，一个字符就是两个字节。一个汉字算两个英文字符的时代已经快过去了。 unicode同样也不完美，这里就有两个的问题，一个是，如何才能区别unicode和ascii？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储空间来说是极大的浪费，文本文件的大小会因此大出二三倍，这是难以接受的。 unicode在很长一段时间内无法推广，直到互联网的出现，为解决unicode如何在网络上传输的问题，于是面向传输的众多 UTF（UCS Transfer Format）标准出现了，顾名思义，UTF-8就是每次8个位传输数据，而UTF-16就是每次16个位。UTF-8就是在互联网上使用最广的一种unicode的实现方式，这是为传输而设计的编码，并使编码无国界，这样就可以显示全世界上所有文化的字符了。 UTF-8最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度，当字符在ASCII码的范围时，就用一个字节表示，保留了ASCII字符一个字节的编码做为它的一部分，注意的是unicode一个中文字符占2个字节，而UTF-8一个中文字符占3个字节）。从unicode到uft-8并不是直接的对应，而是要过一些算法和规则来转换。 Unicode符号范围 UTF-8编码方式 (十六进制) （二进制） 0000 0000-0000 007F 0xxxxxxx 0000 0080-0000 07FF 110xxxxx 10xxxxxx 0000 0800-0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx 0001 0000-0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx]]></content>
      <categories>
        <category>程序员</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tips]]></title>
    <url>%2F20161026%2Fpython_tips%2F</url>
    <content type="text"><![CDATA[【个人GitBook搬运】python使用的小Tips 读写操作 使用codecs模块 1234import codecsfp1 = codecs.open(filename,'w')fp2 = codecs.open(filename, 'r', 'utf-8')lines = fp2.readlins() 文件关闭 123456try:f = open('/path/to/file', 'r')print f.read()finally:if f:f.close() 更简洁的表达：12with open('/path/to/file', 'r') as f:print f.read() read()&amp;readline()&amp;readlines() read()一次性读取文件的全部内容,read(size)每次最多读取size个字节的内容 readline()可以每次读取一行内容 readlines()一次读取所有内容并按行返回list 总结123import codecswith codecs.open('/Users/michael/gbk.txt', 'r', 'gbk') as f:f.read() 文件操作shutil.copytree(windowsDir, photoDir) 工作目录123import oscurDir = os.getcwd() #获取当前工作目录chDir = os.chdir(path) # 更改工作目录 defaultdict &amp; dict = {}dict = defaultdict(default_factory)就是一个字典，只不过python自动为其键值赋上初值 123456789101112131415&gt;&gt;&gt; s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]&gt;&gt;&gt; d = defaultdict(list)&gt;&gt;&gt; for k, v in s:... d[k].append(v)...&gt;&gt;&gt; d.items()[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]&gt;&gt;&gt; s = 'mississippi'&gt;&gt;&gt; d = defaultdict(int)&gt;&gt;&gt; for k in s:... d[k] += 1...&gt;&gt;&gt; d.items()[('i', 4), ('p', 2), ('s', 4), ('m', 1)] list12[word for word in wordlist if word != '*'] # 生成列表for i,e in enumerate(list): # 取出列表元素以及所在位置 下划线的使用 详解Python中的下划线Python单下划线/双下划线使用总结1234object # public__object__ # special, python system use, user should not define like it__object # private (name mangling during runtime)_object # obey python coding convention, consider it as private 单下划线开始的成员变量叫做保护变量，意思是只有类对象和子类对象自己能访问到这些变量 双下划线开始的是私有成员，意思是只有类对象自己能访问，连子类对象也不能访问到这个数据。 以单下划线开头_foo的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用from xxx import *而导入 以双下划线开头的__foo代表类的私有成员 以双下划线开头和结尾的__foo__代表python里特殊方法专用的标识，如 __init__()代表类的构造函数。 内存释放先del，再显式调用gc.collect() 123import gcdel listgc.collect() conda基础使用 conda create --name env_name python=3.6 activate env_name or source activate env_name deactivate env_name or source deactivate env_name conda remove --nme env_name --all conda info -e conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ and conda config --set show_channel_urls yes 使用镜像提升pip下载速度 让PIP源使用国内镜像，提升下载速度和安装成功率 pypi 镜像使用帮助 window在C:\Users\user_name下创建pip/pip.ini文件，文件内容为： 1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.com pass]]></content>
      <categories>
        <category>程序员</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Tips]]></title>
    <url>%2F20160930%2Flinux-tips%2F</url>
    <content type="text"><![CDATA[【个人GitBook搬运】Linux使用过程中的一些小Tips 双系统修改开机引导1sudo vim /boot/grub/grub.cfg set default=”0”修改为set default=”x”——x为第x个选项，Ubuntu下一般为4 set timeout=10修改为set timeout=3——默认三秒 vim下通过”/“查找，”n”寻找下一个，修改查找到的第一个set timeout=10 环境变量 /etc/environment:此文件为整个系统设置环境信息，用户登陆是执行。 /etc/profile：此文件为系统的每个用户设置环境信息，用户登陆是执行。 /etc/bashrc:此文件为系统的shell终端设置环境信息，shell打开是执行。 ~/.profile(~/.bash_profile):单用户生效的profile ~/.bashrc:单用户生效的bashrc 使用source /etc/environment可以使变量设置在当前窗口立即生效，需注销/重启之后，才能对每个新终端窗口都生效。 错误修改/etc/environment导致无法开机修复 alt +ctrl+f1进入命令模式 1/usr/bin/sudo /usr/bin/vi /etc/environment 删除多余export PATH或修复PATH 1PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&quot; 退出vi并重启系统 1/usr/bin/sudo /sbin/reboot JAVA安装 推荐教程 Oracle JDK下载 解压并放在/usr/lib/java 123sudo mkdir /usr/lib/javasudo mv /home/zydar/下载/jdk* /usr/lib/javasudo tar -xvf jdk-* 配置/etc/profile环境变量 123456JAVA_HOME=/usr/lib/java/jdk1.8.0_101JRE_HOME=$JAVA_HOME/jrePATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOMEexport JRE_HOMEexport PATH 使修改生效 1source /etc/profile 验证 1java -version PPA(Personal Package Archives) 添加PPA1sudo add-apt-repository ppa:user/ppa-name 方法一：设置-&gt;软件和更新-&gt;其他软件-&gt;添加-&gt;输入ppa:user/ppa-name 更新源地址 1sudo apt-get update 删除PPA 1sudo add-apt-repository -r ppa:user/ppa-name 方法二：设置-&gt;软件和更新-&gt;其他软件-&gt;选中删除 e.g.使用PPA安装flux1234sudo add-apt-repository ppa:nathan-renniewaldock/fluxsudo apt-get updatesudo apt-get install fluxguisudo /usr/bin/fluxgui 安装搜狗输入法 下载deb包 双击安装 语言设置里把默认输入法改为fcitx 选择“应用到整个系统” 重启，完成 下载.ssf皮肤文件，双击安装皮肤 Ubuntu安装wine-qqintl 下载ZIP 安装32位依赖库 12sudo apt-get install libgtk2.0-0:i386sudo apt-get install -f 解压ZIP，cd进入wine-qqintl 安装 123sudo dpkg -i wine-qqintl_0.1.3-2_i386.debsudo dpkg -i ttf-wqy-microhei_0.2.0-beta-2_all.debsudo dpkg -i fonts-wqy-microhei_0.2.0-beta-2_all.deb Dash中搜索QQ，点击启动 不要试图修改默认设置 不要试图修改默认设置 不要试图修改默认设置 安装扁平主题Flatabulous Flatabulous:超级好看的Ubuntu扁平主题GitHub上官方文档 安装Unity Tweak Tool 1sudo apt-get install unity-tweak-tool 下载Flatabulous源码 1git clone https://github.com/anmoljagetia/Flatabulous 或到GitHub上下载ZIP 移动到/usr/share/themes/下 1sudo mv Flatabulous /usr/share/themes/ 安装扁平图标 123sudo add-apt-repository ppa:noobslab/iconssudo apt-get updatesudo apt-get install ultra-flat-icons Dash中启动Unity Tweak Tool，修改主题与图标 sublime text 3安装与中文兼容优化内存 swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。两个极端，对于ubuntu的默认设置，这个值等于60，建议修改为10。 1234cat /proc/sys/vm/swappiness # 查看swappiness，默认60sudo sysctl vm.swappiness=10 # 临时修改sudo gedit /etc/sysctl.conf文档末尾追加vm.swappiness=10 Ubuntu图形界面卡死 重启 关闭图形界面 ctrl+alt+f1转到tty1 ps -t tty7查看进程号 找到Xorg进程的PID号xxx sudo kill xxx 关闭卡死进程 检测到系统程序出现问题 sudo rm /var/crash/*]]></content>
      <categories>
        <category>程序员</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark搭建]]></title>
    <url>%2F20160927%2Fspark_build%2F</url>
    <content type="text"><![CDATA[【个人GitBook搬运】本地搭建Spark JAVA安装与环境搭建ssh安装与测试 单机Spark可略过略 hadoop安装与配置（可跳过） Apache Hadoop下载（binary文件） 解压放到/home/zydar/software下(不建议放在/usr下) 配置JAVA_HOME 12#export JAVA_HOME=$&#123;JAVA_HOME&#125;export JAVA_HOME=/usr/lib/java/jdk1.8.0_101 配置/etc/profile：export HADOOP_HOME,PATH追加:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin 12345678JAVA_HOME=/usr/lib/java/jdk1.8.0_101JRE_HOME=$JAVA_HOME/jreHADOOP_HOME=/usr/local/bin/hadoop-2.7.3PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/binexport JAVA_HOMEexport JRE_HOMEexport HADOOP_HOMEexport PATH 验证 1hadoop version wordcount测试 1234567cd $HADOOP_HOMEsudo mkdir inputcp etc/hadoop/* inputhadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount input /home/zydar/outputcat /home/zydar/output/*rm -r /home/zydar/outputrm -r input 伪分布Hadoop配置 配置core-site.xml，hdfs-site.xml，yarn-site.xml，mapred-site.xml 123456789101112131415161718192021222324252627282930313233343536【core-site.xml】&lt;configuration&gt;&lt;property&gt;&lt;name&gt;fs.default.name&lt;/name&gt;&lt;value&gt;hdfs://localhost:8082&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;【hdfs-site.xml】&lt;configuration&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;/usr/local/hadoop/hadoop-2.6.4/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;/usr/local/hadoop/hadoop-2.6.4/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;【yarn-site.xml】&lt;configuration&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux.services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;【mapred-site.xml】(cp mapred-site.xml.template mapred-site.xml)&lt;configuration&gt;&lt;property&gt;&lt;name&gt;mapreduce.framwork.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 格式化 1bin/hadoop namenode -format 启动/关闭hadoop 1start-all.sh/stop-all.sh jps查看JAVA进程 查看hadoop localhost:50070localhost:8088/clusterhdfs dfsadmin -report 伪分布wordcount测试123456hdfs dfs -mkdir -p inputhdfs dfs -put etc/hadoop inputhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar wordcount input/hadoop outputhdfs dfs -cat output/*hdfs dfs -rm -r inputhdfs dfs -rm -r output Spark安装与配置 Spark下载 解压放到/home/zydar/software下(不建议放在/usr下) 配置/etc/profile：export SPARK_HOME,PATH追加:$SPARK_HOME/bin 配置环境 12cp ./conf/spark-env.sh.template ./conf/spark-env.shvim ./conf/spark-env.sh export JAVA_HOME=/usr/lib/java/jdk1.8.0_101export SPARK_MASTER_IP=125.216.238.149export SPARK_WORKER_MEMORY=2gexport HADOOP_CONF_DIR=/home/zydar/software/hadoop-2.7.3/etc/hadoop 启动Spark 1./sbin/start-all.sh localhost:8080查看Spark集群 新建job（localhost:4040）1234pyspark --master spark://zydar-HP:7077 --name czh --executor-memory 1G --total-executor-cores 2&gt;&gt;&gt; textFile = sc.textFile(&quot;file:///home/zydar/software/spark-2.0.0/README.md&quot;)&gt;&gt;&gt; textFile.count()&gt;&gt;&gt; textFile.filter(lambda line: line.split(&apos; &apos;)).map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b).map(lambda (a,b): (b,a)).sortByKey(False).map(lambda (a,b): (b,a)).collect() Spark IDE开发环境 配置/etc/profile：export PYTHONPATH1PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip Spark on Pycharm 下载Python（推荐.edu账号注册免费使用Professional版） 解压放到/home/zydar/software下(不建议放在/usr下) 运行 1./bin/pycharm.sh 测试代码 123456789from pyspark import SparkContext,SparkConf#conf = SparkConf().setAppName(&quot;YOURNAME&quot;).setMaster(&quot;local[*]&quot;)conf = SparkConf().setAppName(&quot;YOURNAME&quot;).setMaster(&quot;spark://zydar-HP:7077&quot;).set(&quot;spark.executor.memory&quot;, &quot;1g&quot;).set(&quot;spark.cores.max&quot;, &quot;2&quot;)sc = SparkContext(conf=conf)localFile = &quot;file:///home/zydar/software/spark-2.0.0/README.md&quot;hdfsFile = &quot;README.md&quot;hdfsFile1 = &quot;/user/zydar/README.md&quot;textFile = sc.textFile(localFile)print textFile.count() Spark配置（官方） Spark on Ipython Notebook Ipython Notebook安装与配置 123apt-get install ipython#安装ipythonapt-get install ipython-notebook#安装ipython notebookipython profile create spark#创建spark的config 记下生成的路径/home/zydar/.ipython/profile_spark/ipython_notebook_config.py 进入ipython设置密码 123ipythonIn [1]:from IPython.lib import passwdIn [2]:passwd() 记下返回的sha1 进入ipython_notebook_config.py文件 123c.NotebookApp.password = u&apos;sha1:67c34dbbc0f8:a96f9c64adbf4c58f2e71026a4bffb747d777c5a&apos;c.FileNotebookManager.notebook_dir = u&apos;/home/zydar/software/data/ipythonNotebook&apos;# c.NotebookApp.open_browser = False 打开Ipython Notebook 1ipython notebook --profile=spark 测试代码（同Pycharm）]]></content>
      <categories>
        <category>程序员</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
</search>
